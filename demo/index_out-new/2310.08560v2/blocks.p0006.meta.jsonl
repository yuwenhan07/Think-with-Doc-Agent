{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p6:b0001", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 6, "block_id": "p6:b0001", "type": "figure", "text": "Figure 5. Document QA task performance. MemGPT's performance is unaffected by increased context length. Methods such as truncation can extend the effective context lengths of fixed length models such as GPT-4, but such compression methods will lead to performance degradation as the necessary context grows. Running MemGPT with GPT-4 and GPT-4 Turbo have equivalent results on this task.", "asset_path": "../demo/chunks-new/2310.08560v2/page_0006/p0006_6-ocr-region-0001.png", "bbox_px": [85, 126, 578, 550], "crop_work_size": [1216, 1600], "span_id": "6:ocr:region:0001", "section_type": "methodology", "page_section": "3.2. MemGPT for document analysis", "section_relevance": 1.1, "page_image_path": "/Users/yuwenhan/Library/Mobile Documents/com~apple~CloudDocs/Documents/üêü/ÁßëÁ†î/Learn.agent/DocAgent/demo/imgs/2310.08560v2/page_0006.png", "source": "ocr_span"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p6:b0002", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 6, "block_id": "p6:b0002", "type": "figure", "text": "Figure 6. An example of MemGPT (left) solving the document QA task. A database of Wikipedia documents is uploaded to archival storage. MemGPT queries archival storage via function calling, which pulls paginated search results into main context.", "asset_path": "../demo/chunks-new/2310.08560v2/page_0006/p0006_6-ocr-region-0002.png", "bbox_px": [627, 126, 1124, 550], "crop_work_size": [1216, 1600], "span_id": "6:ocr:region:0002", "section_type": "methodology", "page_section": "3.2. MemGPT for document analysis", "section_relevance": 1.1, "page_image_path": "/Users/yuwenhan/Library/Mobile Documents/com~apple~CloudDocs/Documents/üêü/ÁßëÁ†î/Learn.agent/DocAgent/demo/imgs/2310.08560v2/page_0006.png", "source": "ocr_span"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p6:b0003", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 6, "block_id": "p6:b0003", "type": "text", "text": "Figure 5. Document QA task performance. MemGPT's performance is unaffected by increased context length. Methods such as truncation can extend the effective context lengths of fixed length models such as GPT-4, but such compression methods will lead to performance degradation as the necessary context grows. Running MemGPT with GPT-4 and GPT-4 Turbo have equivalent results on this task.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "section_type": "methodology", "page_section": "3.2. MemGPT for document analysis", "section_relevance": 1.1, "page_image_path": "/Users/yuwenhan/Library/Mobile Documents/com~apple~CloudDocs/Documents/üêü/ÁßëÁ†î/Learn.agent/DocAgent/demo/imgs/2310.08560v2/page_0006.png", "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p6:b0004", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 6, "block_id": "p6:b0004", "type": "text", "text": "first response in the following session. We report the CSIM scores of MemGPT's openers in Table 3. We test several variations of MemGPT using different base LLMs.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "section_type": "methodology", "page_section": "3.2. MemGPT for document analysis", "section_relevance": 1.1, "page_image_path": "/Users/yuwenhan/Library/Mobile Documents/com~apple~CloudDocs/Documents/üêü/ÁßëÁ†î/Learn.agent/DocAgent/demo/imgs/2310.08560v2/page_0006.png", "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p6:b0005", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 6, "block_id": "p6:b0005", "type": "text", "text": "**MemGPT utilizes memory to increase engaging:** As seen in Table 3, MemGPT is able to craft engaging openers that perform similarly to and occasionally exceed the hand-written human openers. We observe that MemGPT tends to craft openers that are both more verbose and cover more aspects of the persona information than the human baseline. Additionally, we can see the storing information in working context is key to generating engaging openers.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "section_type": "methodology", "page_section": "3.2. MemGPT for document analysis", "section_relevance": 1.1, "page_image_path": "/Users/yuwenhan/Library/Mobile Documents/com~apple~CloudDocs/Documents/üêü/ÁßëÁ†î/Learn.agent/DocAgent/demo/imgs/2310.08560v2/page_0006.png", "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p6:b0006", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 6, "block_id": "p6:b0006", "type": "text", "text": "## 3.2. MemGPT for document analysis", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "section_type": "methodology", "page_section": "3.2. MemGPT for document analysis", "section_relevance": 1.1, "page_image_path": "/Users/yuwenhan/Library/Mobile Documents/com~apple~CloudDocs/Documents/üêü/ÁßëÁ†î/Learn.agent/DocAgent/demo/imgs/2310.08560v2/page_0006.png", "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p6:b0007", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 6, "block_id": "p6:b0007", "type": "text", "text": "Document analysis also faces challenges due to the limited context windows of today's transformer models. As shown in Table 1, both open and closed source models suffer from constrained context length (up to 128k tokens for OpenAI's models). However many documents easily surpass these lengths; for example, legal or financial documents such as Annual Reports (SEC Form 10-K) can easily pass the million token mark. Moreover, many real document analysis tasks require drawing connections across multiple such lengthy documents. Anticipating these scenarios, it becomes difficult to envision blindly scaling up context as a solution to the fixed-context problem. Recent research (Liu et al., 2023a) also raises doubts about the utility of simply scaling contexts, since they find uneven attention distributions in large context models (the model is more capable of recalling information at the beginning or end of its context window, vs tokens in the middle). To enable reasoning across documents, more flexible memory architectures like MemGPT are needed.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "section_type": "methodology", "page_section": "3.2. MemGPT for document analysis", "section_relevance": 1.1, "page_image_path": "/Users/yuwenhan/Library/Mobile Documents/com~apple~CloudDocs/Documents/üêü/ÁßëÁ†î/Learn.agent/DocAgent/demo/imgs/2310.08560v2/page_0006.png", "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p6:b0008", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 6, "block_id": "p6:b0008", "type": "text", "text": "### 3.2.1. MULTI-DOCUMENT QUESTION-ANSWERING.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "section_type": "methodology", "page_section": "3.2. MemGPT for document analysis", "section_relevance": 1.1, "page_image_path": "/Users/yuwenhan/Library/Mobile Documents/com~apple~CloudDocs/Documents/üêü/ÁßëÁ†î/Learn.agent/DocAgent/demo/imgs/2310.08560v2/page_0006.png", "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p6:b0009", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 6, "block_id": "p6:b0009", "type": "text", "text": "To evaluate MemGPT's ability to analyze documents, we benchmark MemGPT against fixed-context baselines on the retriever-reader document QA task from Liu et al. (2023a). In this task, a question is selected from the NaturalQuestions-Open dataset, and a retriever selects relevant Wikipedia documents for the question. A reader model (the LLM) is then fed these documents as input, and is asked to use the provided documents to answer the question. Similar to Liu et al. (2023a), we evaluate reader accuracy as the number of retrieved documents $K$ increases.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "section_type": "methodology", "page_section": "3.2. MemGPT for document analysis", "section_relevance": 1.1, "page_image_path": "/Users/yuwenhan/Library/Mobile Documents/com~apple~CloudDocs/Documents/üêü/ÁßëÁ†î/Learn.agent/DocAgent/demo/imgs/2310.08560v2/page_0006.png", "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p6:b0010", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 6, "block_id": "p6:b0010", "type": "text", "text": "In our evaluation setup, both the fixed-context baselines and MemGPT use the same retriever, which selects the top $K$ documents according using similarity search (cosine distance) on OpenAI's `text-embedding-ada-002` embeddings. We use MemGPT's default storage settings which uses PostgreSQL for archival memory storage with vector search enabled via the pgvector extension. We precompute embeddings and load them into the database, which uses an HNSW index to enable approximate, sub-second query times. In MemGPT, the entire embedding document set is loaded into archival storage, and the retriever naturally emerges via the archival storage search functionality (which performs vector search based on cosine similarity). In the fixed-context baselines, the top-$K$ documents are fetched using the retriever independently from the LLM inference, similar to the original retriever-reader setup in Liu et al. (2023a).", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "section_type": "methodology", "page_section": "3.2. MemGPT for document analysis", "section_relevance": 1.1, "page_image_path": "/Users/yuwenhan/Library/Mobile Documents/com~apple~CloudDocs/Documents/üêü/ÁßëÁ†î/Learn.agent/DocAgent/demo/imgs/2310.08560v2/page_0006.png", "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p6:b0011", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 6, "block_id": "p6:b0011", "type": "text", "text": "We use a dump of Wikipedia from late 2018, following past work on NaturalQuestions-Open (Izacard & Grave, 2020;", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "section_type": "methodology", "page_section": "3.2. MemGPT for document analysis", "section_relevance": 1.1, "page_image_path": "/Users/yuwenhan/Library/Mobile Documents/com~apple~CloudDocs/Documents/üêü/ÁßëÁ†î/Learn.agent/DocAgent/demo/imgs/2310.08560v2/page_0006.png", "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p6:b0012", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 6, "block_id": "p6:b0012", "type": "text", "text": "Figure 6. An example of MemGPT (left) solving the document QA task. A database of Wikipedia documents is uploaded to archival storage. MemGPT queries archival storage via function calling, which pulls paginated search results into main context.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "section_type": "methodology", "page_section": "3.2. MemGPT for document analysis", "section_relevance": 1.1, "page_image_path": "/Users/yuwenhan/Library/Mobile Documents/com~apple~CloudDocs/Documents/üêü/ÁßëÁ†î/Learn.agent/DocAgent/demo/imgs/2310.08560v2/page_0006.png", "source": "ocr_md_rule"}
