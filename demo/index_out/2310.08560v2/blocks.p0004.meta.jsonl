{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0001", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0001", "type": "text", "text": "# MemGPT: Towards LLMs as Operating Systems", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0002", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0002", "type": "text", "text": "## Table 1. Comparing context lengths of commonly used models and LLM APIs (data collected 1/2024)", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0003", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0003", "type": "text", "text": "*Approximate message count assuming a preprompt of 1k tokens, and an average message size of ~50 tokens (~250 characters). 'Open' means the model is open-source or open-weights (vs only available behind an API).*", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0004", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0004", "type": "text", "text": "| Model / API name          | Open? | Context Window Tokens | *Messages |\n|---------------------------|-------|------------------------|-----------|\n| Llama (1)                 | âœ“     | 2k                     | 20        |\n| Llama 2                   | âœ“     | 4k                     | 60        |\n| GPT-3.5 Turbo (release)   | âœ—     | 4k                     | 60        |\n| Mistral 7B                | âœ“     | 8k                     | 140       |\n| GPT-4 (release)           | âœ—     | 8k                     | 140       |\n| GPT-3.5 Turbo             | âœ—     | 16k                    | 300       |\n| GPT-4                     | âœ—     | 32k                    | ~600      |\n| Claude 2                  | âœ—     | 100k                   | ~2000     |\n| GPT-4 Turbo               | âœ—     | 128k                   | ~2600     |\n| Yi-34B-200k               | âœ“     | 200k                   | ~4000     |", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0005", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0005", "type": "text", "text": "---", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0006", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0006", "type": "text", "text": "## Figure 4. An example conversation snippet where MemGPT (left) updates stored information.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0007", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0007", "type": "text", "text": "Here the information is stored in working context memory (located within the prompt tokens).", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0008", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0008", "type": "text", "text": "> **User**: How's James doing? Any special plans today?  \n> **MemGPT**: `working_context.replace(\"Boyfriend named James\", \"Ex-boyfriend named James\")`  \n> **User**: actually james and i broke up  \n> **MemGPT**: Sorry to hear that - hope you're OK ðŸ’”", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0009", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0009", "type": "text", "text": "---", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0010", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0010", "type": "text", "text": "## 2.4. Control flow and function chaining", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0011", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0011", "type": "text", "text": "In MemGPT, *events* trigger LLM inference: events are generalized inputs to MemGPT and can consist of user messages (in chat applications), system messages (e.g., main context capacity warnings), user interactions (e.g., an alert that a user just logged in, or an alert that they finished uploading a document), and timed events that are run on a regular schedule (allowing MemGPT to run â€˜unpromptedâ€™ without user intervention). MemGPT processes events with a parser to convert them into plain text messages that can be appended to main context and eventually be fed as input into the LLM processor.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0012", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0012", "type": "text", "text": "Many practical tasks require calling multiple functions in sequence, for example, navigating through multiple pages of results from a single query or collating data from different documents in main context from separate queries. Function chaining allows MemGPT to execute multiple function calls sequentially before returning control to the user. In MemGPT, functions can be called with a special flag that requests control be immediately returned to the processor after the requested function completes execution. If this flag is present, MemGPT will add the function output to main context and (as opposed to pausing processor execution). If this flag is not present (a *yield*), MemGPT will not run the LLM processor until the next external event trigger (e.g., a user message or scheduled interrupt).", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0013", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0013", "type": "text", "text": "---", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0014", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0014", "type": "text", "text": "## 3. Experiments", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0015", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0015", "type": "text", "text": "We assess MemGPT in two long-context domains: conversational agents and document analysis. For conversational agents, we expand the existing Multi-Session Chat dataset (Xu et al., 2021) and introduce two new dialogue tasks that evaluate an agentâ€™s ability to retain knowledge across long conversations. For document analysis, we benchmark MemGPT on existing tasks from (Liu et al., 2023a) for question answering and key-value retrieval over lengthy documents. We also propose a new nested key-value retrieval task requiring collating information across multiple data sources, which tests the ability of an agent to collate information from multiple data sources (multi-hop retrieval). We publicly release our augmented MSC dataset, nested KV retrieval dataset, and a dataset of embeddings for 20M Wikipedia articles to facilitate future research. Our code for the benchmarks is available at [https://research.memgpt.ai](https://research.memgpt.ai).", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0016", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0016", "type": "text", "text": "### Implementation details", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0017", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0017", "type": "text", "text": "When discussing OpenAI models, unless otherwise specified:\n- â€˜GPT-4 Turboâ€™ refers to the specific `gpt-4-1106-preview` model endpoint (context window of 128,000),\n- â€˜GPT-4â€™ refers to `gpt-4-0613` (context window of 8,192),\n- â€˜GPT-3.5 Turboâ€™ refers to `gpt-3.5-turbo-1106` (context window of 16,385).", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0018", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0018", "type": "text", "text": "In experiments, we run MemGPT with all baseline models (GPT-4, GPT-4 Turbo, and GPT 3.5) to show how the underlying model performance affects MemGPTâ€™s.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0019", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0019", "type": "text", "text": "---", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0020", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0020", "type": "text", "text": "## 3.1. MemGPT for conversational agents", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0021", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0021", "type": "text", "text": "Conversational agents like virtual companions and personalized assistants aim to engage users in natural, long-term interactions, potentially spanning weeks, months, or even years. This creates challenges for models with fixed-length contexts, which can only reference a limited history of the conversation. An â€˜infinite contextâ€™ agent should seamlessly handle continuous exchanges without boundary or reset.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0022", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0022", "type": "text", "text": "When conversing with a user, such an agent must satisfy two key criteria:\n1. **Consistency** - The agent should maintain conversational coherence. New facts, preferences, and events mentioned should align with prior statements from both the user and agent.\n2. **Engagement** - The agent should draw on long-term knowledge about the user to personalize interactions.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:b0023", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "block_id": "p4:b0023", "type": "text", "text": "---\n**Date**: February 14\n**Page**: 4", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
