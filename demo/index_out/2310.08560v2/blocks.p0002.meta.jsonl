{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p2:b0001", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 2, "block_id": "p2:b0001", "type": "text", "text": "# MemGPT: Towards LLMs as Operating Systems", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p2:b0002", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 2, "block_id": "p2:b0002", "type": "text", "text": "## Figure 1: MemGPT (left) writes data to persistent memory after it receives a system alert about limited context space.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p2:b0003", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 2, "block_id": "p2:b0003", "type": "text", "text": "**February 7**", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p2:b0004", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 2, "block_id": "p2:b0004", "type": "text", "text": "> **User:** How was your day today?  \n> **MemGPT:** fun my bf james baked me a birthday cake  \n> **User:** Oh wow, happy birthday! ðŸŽ‰  \n> **System Alert: Memory Pressure**  \n> `working_context.append(\"Birthday is February 7\")`  \n> `working_context.append(\"Boyfriend named James\")`", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p2:b0005", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 2, "block_id": "p2:b0005", "type": "text", "text": "---", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p2:b0006", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 2, "block_id": "p2:b0006", "type": "text", "text": "## Figure 2: MemGPT (left) can search out-of-context data to bring relevant information into the current context window.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p2:b0007", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 2, "block_id": "p2:b0007", "type": "text", "text": "**February 7**", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p2:b0008", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 2, "block_id": "p2:b0008", "type": "text", "text": "> **User:** Did you do anything else to celebrate your birthday? ðŸ˜„  \n> **MemGPT:** yeah we went to six flags!  \n> `recall_storage.search(\"six flags\")`  \n> **Results:**  \n> - [01/24/2024] \"lol yeah six flags\"  \n> - [01/14/2024] \"i love six flags been like 100 times\"  \n> - [10/12/2023] \"james and I actually first met at six flags\"  \n> **User:** Did you go with James? It's so cute how both met there!", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p2:b0009", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 2, "block_id": "p2:b0009", "type": "text", "text": "---", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p2:b0010", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 2, "block_id": "p2:b0010", "type": "text", "text": "with *virtual memory*, which provides an illusion of there being more memory resources than are actually available in physical (i.e., main) memory by the OS paging overflow data to disk and retrieving data (via a page fault) back into memory when accessed by applications. To provide a similar illusion of longer context length (analogous to virtual memory), we allow the LLM to manage what is placed in its own context (analogous to physical memory) via an â€˜LLM OSâ€™, which we call MemGPT. MemGPT enables the LLM to retrieve relevant historical data missing from what is placed in-context, and also evict less relevant data from context and into external storage systems. Figure 3 illustrates the components of MemGPT.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p2:b0011", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 2, "block_id": "p2:b0011", "type": "text", "text": "The combined use of a memory-hierarchy, OS functions and event-based control flow allow MemGPT to handle unbounded context using LLMs that have finite context windows. To demonstrate the utility of our new OS-inspired LLM system, we evaluate MemGPT on two domains where the performance of existing LLMs is severely limited by finite context: document analysis, where the length of standard text files can quickly exceed the input capacity of modern LLMs, and conversational agents, where LLMs bound by limited conversation windows lack context awareness, persona consistency, and long-term memory during extended conversations. In both settings, MemGPT is able to overcome the limitations of finite context to outperform existing LLM-based approaches.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p2:b0012", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 2, "block_id": "p2:b0012", "type": "text", "text": "## 2. MemGPT (MemoryGPT)", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p2:b0013", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 2, "block_id": "p2:b0013", "type": "text", "text": "MemGPTâ€™s OS-inspired multi-level memory architecture delineates between two primary memory types: **main context** (analogous to main memory/physical memory/RAM) and **external context** (analogous to disk memory/disk storage). Main context consists of the LLM *prompt tokens*â€”anything in main context is considered *in-context* and can be accessed by the LLM processor during inference. External context refers to any information that is held outside of the LLMs fixed context window. This *out-of-context* data must always be explicitly moved into main context in order for it to be passed to the LLM processor during inference. MemGPT provides function calls that the LLM processor to manage its own memory without any user intervention.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p2:b0014", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 2, "block_id": "p2:b0014", "type": "text", "text": "### 2.1. Main context (prompt tokens)", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p2:b0015", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 2, "block_id": "p2:b0015", "type": "text", "text": "The prompt tokens in MemGPT are split into three contiguous sections: the **system instructions**, **working context**, and **FIFO Queue**. The system instructions are read-only (static) and contain information on the MemGPT control flow, the intended usage of the different memory levels, and instructions on how to use the MemGPT functions (e.g. how to retrieve out-of-context data). Working context is a fixed-size read/write block of unstructured text, writeable only via MemGPT function calls. In conversational settings, working context is intended to be used to store key facts, preferences, and other important information about the user and the persona the agent is adopting, allowing the agent to converse fluently with the user. The FIFO queue stores a rolling history of messages, including messages between the agent and user, as well as system messages (e.g. memory warnings) and function call inputs and outputs. The first index in the FIFO queue stores a system message containing a recursive summary of messages that have been evicted from the queue.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p2:b0016", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 2, "block_id": "p2:b0016", "type": "text", "text": "### 2.2. Queue Manager", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p2:b0017", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 2, "block_id": "p2:b0017", "type": "text", "text": "The queue manager manages messages in *recall storage* and the *FIFO queue*. When a new message is received by the system, the queue manager appends the incoming messages to the FIFO queue, concatenates the prompt tokens and triggers the LLM inference to generate LLM output (the completion tokens). The queue manager writes both the incoming message and the generated LLM output to recall storage (the MemGPT message database). When messages in recall storage are retrieved via a MemGPT function call, the queue manager appends them to the back of the FIFO queue.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
