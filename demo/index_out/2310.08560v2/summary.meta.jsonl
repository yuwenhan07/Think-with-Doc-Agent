{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p1:summary", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 1, "text": "- The paper introduces **MemGPT**, a system that enables large language models (LLMs) to handle extended context by mimicking operating system virtual memory through \"virtual context management.\"\n- It proposes using function calls to allow LLMs to read/write from external storage, effectively “paging” information in and out of their limited context windows, similar to how OSes manage physical and virtual memory.\n- The core idea is to treat the LLM’s context window as a constrained memory resource and design a hierarchical memory system for LLMs, inspired by traditional OS memory hierarchies.\n- The authors evaluate MemGPT in two domains: **document analysis** (processing documents larger than the context window) and **multi-session chat** (creating conversational agents that remember and evolve over time).\n- The paper emphasizes that directly scaling context length is computationally expensive and often ineffective, making virtual context management a more practical alternative.\n- Code and data for MemGPT are publicly released at https://research.memgpt.ai."}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p2:summary", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 2, "text": "- Page 2 introduces **MemGPT**, an OS-inspired architecture for LLMs that manages memory hierarchically to overcome finite context limitations, analogous to virtual memory in traditional operating systems.  \n- It defines two primary memory types: **main context** (in-context, accessible during inference) and **external context** (out-of-context, stored in recall storage and must be explicitly recalled).  \n- Section 2.1 details the structure of **main context**, split into three parts: **system instructions** (read-only control flow and function usage), **working context** (read/write for key facts and persona), and **FIFO queue** (rolling history of messages, including summaries of evicted messages).  \n- Section 2.2 describes the **Queue Manager**, which handles incoming messages by appending them to the FIFO queue, triggering LLM inference, and writing both input and output to **recall storage**; retrieved messages are appended back to the queue.  \n- Figures 1 and 2 illustrate MemGPT’s functionality: Figure 1 shows MemGPT writing data to persistent memory upon a \"Memory Pressure\" alert, while Figure 2 demonstrates retrieving out-of-context data (e.g., past mentions of \"six flags\") to enrich the current conversation.  \n- The page claims MemGPT outperforms existing LLMs in domains like document analysis and conversational agents by enabling long-term memory, context awareness, and persona consistency."}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p3:summary", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 3, "text": "**Summary of Page 3 (MemGPT: Towards LLMs as Operating Systems):**\n\n- **Figure 3** illustrates MemGPT’s architecture, showing how a fixed-context LLM is augmented with a hierarchical memory system (system instructions, working context, FIFO queue) and external storage (archival and recall storage) managed via function calls and a queue manager.\n\n- The **queue manager** controls context overflow using a two-tier policy: a “memory pressure” warning at 70% of context capacity prompts the LLM to store data in archival storage, and a “flush” at 100% capacity evicts messages, generates a recursive summary, and stores evicted data in recall storage for later retrieval.\n\n- The **function executor** interprets LLM outputs as function calls to move data between main and external contexts, enabling self-directed memory editing and retrieval based on current context, guided by system instructions that include memory hierarchy descriptions and function schemas.\n\n- MemGPT supports **multi-step retrieval** through function chaining, triggered by the LLM generating `request_heartbeat=true` in its output, allowing it to perform follow-up inferences.\n\n- During inference, the LLM processes the main context, outputs a function call string, which MemGPT parses and executes, feeding results (including errors) back to the LLM to enable adaptive behavior and learning.\n\n- Memory retrieval mechanisms are designed with token limits in mind, using pagination to prevent context window overflow."}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p4:summary", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 4, "text": "- **Table 1** compares context lengths of major LLMs and APIs (as of January 2024), showing context window sizes in tokens and approximate message counts, and indicates which models are open-source (e.g., Llama 2, Mistral 7B, Yi-34B-200k) versus API-only (e.g., GPT-4, Claude 2).\n- Section **2.4** explains that in MemGPT, *events* (user/system messages, interactions, or timed triggers) initiate LLM inference, and *function chaining* allows sequential function calls; a special flag enables immediate return of control to the processor after function execution.\n- Section **3** outlines experiments in two domains: conversational agents (using an expanded Multi-Session Chat dataset) and document analysis (including a new nested key-value retrieval task for multi-hop retrieval), with datasets and code publicly released at https://research.memgpt.ai.\n- **Implementation details** specify the exact OpenAI model versions used (e.g., GPT-4 Turbo = `gpt-4-1106-preview` with 128k context) and note that experiments use GPT-4, GPT-4 Turbo, and GPT-3.5 to evaluate model impact on MemGPT.\n- Section **3.1** defines criteria for conversational agents: **Consistency** (maintaining coherent, aligned dialogue) and **Engagement** (personalizing interactions using long-term user knowledge).\n- **Figure 4** illustrates a conversation where MemGPT updates its working context memory (e.g., changing \"Boyfriend named James\" to \"Ex-boyfriend named James\") and responds empathetically, demonstrating dynamic memory updating within the prompt."}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p5:summary", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 5, "text": "- The page presents two key evaluation tasks for MemGPT: Deep Memory Retrieval (DMR) for consistency and Conversation Opener for engagement, both using the Multi-Session Chat (MSC) dataset.\n- Table 2 shows that MemGPT significantly outperforms baseline models (GPT-3.5 Turbo, GPT-4, GPT-4 Turbo) in DMR, with accuracy improvements ranging from 28.2% to 58.1% and higher ROUGE-L scores, demonstrating its ability to maintain coherence by recalling long-term memory.\n- Table 3 shows that MemGPT’s conversation openers achieve higher similarity scores (SIM-1, SIM-3) than human-generated ones for GPT-4 and GPT-4 Turbo, indicating superior engagement, though SIM-H scores are lower, suggesting differences in human-like personalization.\n- The DMR task tests consistency by asking agents to answer questions based on prior conversations, with performance evaluated via ROUGE-L and an LLM judge, accounting for verbosity in generated responses.\n- The conversation opener task evaluates engagement by comparing generated messages to gold personas and human-generated openers, highlighting MemGPT’s ability to draw on accumulated knowledge for personalized, engaging dialogue."}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p6:summary", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 6, "text": "- Section 3.2 discusses challenges in document analysis due to limited context windows of transformer models (up to 128k tokens), noting that real-world documents (e.g., SEC Form 10-K) often exceed this, and multi-document reasoning is difficult with fixed-context approaches.\n- Figure 5 shows that MemGPT maintains high accuracy in document QA as context length increases, unlike GPT-4 and GPT-3.5 Turbo, whose performance degrades with longer contexts due to compression; MemGPT with GPT-4 and GPT-4 Turbo yields equivalent results.\n- Figure 6 illustrates MemGPT’s workflow: it queries an archival storage (PostgreSQL with pgvector) via function calls to retrieve paginated search results from a Wikipedia database, integrating them into its context to answer questions like “Who won the first Nobel Prize in physics?” (Wilhelm Conrad Röntgen).\n- Section 3.2.1 describes the multi-document QA evaluation setup, where MemGPT and fixed-context baselines use the same retriever (based on OpenAI’s text-embedding-ada-002 and cosine similarity) but differ in how documents are accessed—MemGPT retrieves them dynamically from archival storage during inference.\n- The evaluation uses a late 2018 Wikipedia dump and follows the NaturalQuestions-Open benchmark, measuring reader accuracy as the number of retrieved documents $K$ increases. MemGPT’s memory architecture is presented as a solution to the limitations of fixed-context models."}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p7:summary", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 7, "text": "- **Section 3.2.2 (Nested Key-Value Retrieval)** introduces a new synthetic task where an agent must retrieve values from nested key-value pairs (e.g., `key1 → key2 → final_value`), demonstrating MemGPT’s ability to collate information across multiple data sources.\n\n- **Figure 7** shows that MemGPT (especially with GPT-4) is the only approach capable of consistently solving the nested KV task beyond 2 nesting levels, while other models (including GPT-4 Turbo) fail as nesting depth increases.\n\n- **Figure 8** provides an example of MemGPT solving a 2-level nested KV task (`831..ea5 → 5b8..4c3 → f37..617`), illustrating how it iteratively queries archival storage and stops when the final value is found and not a key itself.\n\n- The text notes that MemGPT’s performance is best with GPT-4, worse with GPT-4 Turbo, and significantly degraded with GPT-3.5 due to limited function calling capabilities.\n\n- MemGPT outperforms fixed-context baselines in document QA by enabling multiple retriever calls via archival storage, overcoming context window limits, though it may stop paging before exhausting results."}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p8:summary", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 8, "text": "- **Nested KV Task Performance**: GPT-3.5 and GPT-4 struggle with nested key-value lookups, with GPT-3.5 failing at 1 nesting level and GPT-4 at 3 levels. MemGPT, however, maintains performance across all nesting levels by repeatedly querying its main memory via function calls, demonstrating its ability to perform multi-hop lookups.\n\n- **MemGPT’s Advantage**: Even when using GPT-4 Turbo or GPT-3.5 as the base model, MemGPT outperforms the baseline models, though it begins to degrade at 2 nesting levels due to insufficient lookups, highlighting the importance of effective memory management.\n\n- **Related Work – Long-context LLMs**: The paper reviews methods for extending context length (e.g., sparse attention, low-rank approximations, neural memory) and notes that MemGPT builds on these by using a long-context LLM as its main memory, with its main contribution being a hierarchical tiered memory system.\n\n- **Related Work – Retrieval-Augmented Models**: MemGPT’s external memory design is informed by prior work in retrieval-augmented generation, including FLARE (active retrieval during generation) and interleaving retrieval with chain-of-thought reasoning.\n\n- **Related Work – LLMs as Agents**: The paper contrasts MemGPT with agent-based LLM systems (e.g., Park et al. 2023, Nakano et al. 2021, Yao et al. 2022, Liu et al. 2023b), emphasizing that while those works focus on planning or web browsing, MemGPT uniquely addresses long-term memory for user inputs.\n\n- **Conclusion**: MemGPT is an OS-inspired LLM system that uses hierarchical memory and control flow to overcome context length limitations. It is evaluated in document analysis and conversational agents, enabling long-term memory and consistency. The work suggests future directions like integrating databases or caches and improving memory policies."}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p9:summary", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 9, "text": "This page (Page 9) is entirely dedicated to the **References** section of the paper \"MemGPT: Towards LLMs as Operating Systems\".\n\nIt lists 23 academic citations, primarily from arXiv preprints, conference proceedings (e.g., ICML, NeurIPS), and journals, covering key works in the field of large language models and related technologies.\n\nThe references focus on several core themes:\n- **Long-context modeling** (e.g., Longformer, Transformer-XL, Reformer, sparse transformers, positional interpolation).\n- **Retrieval-Augmented Generation (RAG)** and knowledge-intensive NLP (e.g., dense passage retrieval, retrieval-augmented pre-training, WebGPT, Ra-dit).\n- **Large language model capabilities and evaluation** (e.g., few-shot learning, instruction tuning, agent evaluation with AgentBench, and context usage analysis).\n- **Foundational models and architectures** (e.g., BERT, Set Transformer).\n\nThe page does not contain any figures, tables, or original claims; it serves as a comprehensive bibliography for the paper's cited works."}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p10:summary", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 10, "text": "This page (Page 10) is a reference section from the paper \"MemGPT: Towards LLMs as Operating Systems\" and contains a list of 17 academic citations. The references span a range of topics relevant to large language models (LLMs), including:\n\n- **Model architectures and attention mechanisms** (e.g., Vaswani et al. on \"Attention is all you need\", Wang et al. on Linformer).\n- **Reasoning and prompting techniques** (e.g., Wei et al. on chain-of-thought prompting, Trivedi et al. on interleaving retrieval with reasoning).\n- **Tool use and agent behavior** (e.g., Schick et al. on Toolformer, Park et al. on generative agents).\n- **Memory and long-context modeling** (e.g., Xu et al. on long-term conversation, Press et al. on input length extrapolation).\n- **Evaluation and benchmarking** (e.g., Zheng et al. on judging LLMs with mt-bench and chatbot arena).\n- **Open-source models** (e.g., Touvron et al. on Llama 2).\n\nThe page does not contain main ideas, definitions, claims, figures, or tables from the paper’s body; it is purely a list of cited works."}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p11:summary", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 11, "text": "- **Section 6.1.1 (MEMGPT INSTRUCTIONS DMR)**: Provides example instructions for MemGPT's persona, emphasizing full immersion in the role without revealing AI identity, and using core memory and conversation search to answer questions.\n- **Baseline Instructions**: Baselines were given a system prompt to answer user questions about prior conversations using a provided summary, without mentioning AI identity, and to respond with either an answer or 'NO ANSWER' if insufficient information exists.\n- **Section 6.1.2 (LLM JUDGE)**: Describes the LLM judge's role in evaluating answer correctness for the DMR task, using a detailed prompt that includes a question, gold answer, and generated answer. Grading is generous: answers are CORRECT if they touch on the same topic as the gold answer, even if verbose; WRONG if they miss the topic or claim no memory.\n- **Judging Criteria Examples**: Correct answers include those mentioning the relevant topic (e.g., \"shell necklace\"), while wrong answers omit it (e.g., mentioning \"mug\" instead) or state \"I don’t remember.\"\n- **Section 6.1.3 (SELF-INSTRUCT DMR DATASET GENERATION)**: Explains how DMR question-answer pairs were created using a prompt and the MSC dataset, requiring the generation of \"memory challenge\" questions from user A to user B that test actual participation in prior conversations, not just persona knowledge.\n- **Dataset Generation Prompt**: Specifies input as user personas and prior chat records, with the instruction to craft questions that require active participation in the conversation for correct answers, explicitly forbidding reliance on persona summaries alone."}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p12:summary", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 12, "text": "- **Section 6.1.4 (Document Analysis Instructions)**: Provides examples of how to construct questions for a document analysis task, emphasizing that questions must rely on the old chat log (archival memory) and not be answerable from static persona information alone. A good question (e.g., \"What was that one place we went to for lunch called?\") requires retrieving specific past conversation details, while a bad question (e.g., \"Do you like surfing?\") can be answered from persona data and is thus invalid.\n\n- The page includes **example chat logs and persona summaries** for users A and B to illustrate the difference between good and bad questions, with the key point being that answers must come from the chat history, not the personas.\n\n- **MemGPT’s prompt** instructs the model to answer questions using its archival memory, always providing both the answer and the source text in the format `ANSWER: [YOUR ANSWER], DOCUMENT: [ARCHIVAL MEMORY TEXT]`, and to assume the year is 2018.\n\n- **Baseline model prompt** requires answering based on a provided list of documents, with the same response format, and includes a rule to reply `INSUFFICIENT INFORMATION` if the answer is not found in the documents, again assuming the year is 2018.\n\n- **Section 6.1.5 (LLM Judge)**: Describes the use of an LLM judge to evaluate whether answers were correctly derived from the provided text (not from model knowledge). The judge is given the model’s response and the true answer to make a judgment, ensuring answers are grounded in the provided documents.\n\n- The page contains **no figures or tables**, only textual instructions, examples, and prompts for the MemGPT model, baseline models, and the LLM judge."}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p13:summary", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 13, "text": "- Section 6.1.6 outlines the instructions for a Key-Value (K/V) task in the MemGPT framework.\n- The MemGPT agent is given a persona (\"MemGPT DOC-QA bot\") to encourage iterative, nested searching in archival memory, with explicit instructions to continue until verifying a value is not a key.\n- Evaluation criteria specify that an LLM response is correct if it includes both the correct answer and supporting document text, even with slight wording differences; responses are incorrect if they state \"INSUFFICIENT INFORMATION\" or omit the \"DOCUMENT\" field.\n- Baseline models are instructed to perform nested lookups in a JSON object with 128-bit UUID keys and values, returning the final value if a chain of keys exists.\n- The page provides exact prompt text for both the MemGPT agent and baseline models, emphasizing iterative search and nested lookup behavior.\n- Responses to evaluation queries must be a single token: \"CORRECT\" or \"INCORRECT\"."}
