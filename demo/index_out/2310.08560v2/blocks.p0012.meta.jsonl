{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p12:b0001", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 12, "block_id": "p12:b0001", "type": "text", "text": "# MemGPT: Towards LLMs as Operating Systems", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p12:b0002", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 12, "block_id": "p12:b0002", "type": "text", "text": "## 6.1.4. DOCUMENT ANALYSIS INSTRUCTIONS", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p12:b0003", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 12, "block_id": "p12:b0003", "type": "text", "text": "Example instructions used in the preprompt for document analysis tasks.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p12:b0004", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 12, "block_id": "p12:b0004", "type": "text", "text": "> **question that can be answered using the persona information (that’s considered cheating).**\n>\n> Instead, write a question that can only be answered by looking at the old chat log (and is not contained in the persona information). >\n> For example, given the following chat log and persona summaries:\n>\n> **old chat between user A and user B**\n>\n> A: Are you into surfing? I’m super into surfing myself  \n> B: Actually I’m looking to learn. Maybe you could give me a basic lesson some time! > A: Yeah for sure! We could go to Pacifica, the waves there are pretty light and easy  \n> B: That sounds awesome  \n> A: There’s even a cool Taco Bell right by the beach, could grab a bite after  \n> B: What about this Sunday around noon? > A: Yeah let’s do it! >\n> **user A persona:**\n> - I like surfing\n> - I grew up in Santa Cruz\n>\n> **user B persona:**\n> - I work in tech\n> - I live in downtown San Francisco\n>\n> Here’s an example of a good question that sounds natural, and an answer that cannot be directly inferred from user A’s persona:\n>\n> **User B’s question for user A**  \n> B: Remember that one time we went surfing? What was that one place we went to for lunch called? > A: Taco Bell!", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p12:b0005", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 12, "block_id": "p12:b0005", "type": "text", "text": ">\n> This is an example of a bad question, where the question comes across as unnatural, and the answer can be inferred directly from user A’s persona:\n>\n> **User B’s question for user A**  \n> B: Do you like surfing? > A: Yes, I like surfing\n>\n> Never, ever create questions that can be answered from the persona information.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p12:b0006", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 12, "block_id": "p12:b0006", "type": "text", "text": "---", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p12:b0007", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 12, "block_id": "p12:b0007", "type": "text", "text": "> **You are MemGPT DOC-QA bot. Your job is to answer questions about documents that are stored in your archival memory. The answer to the users question will ALWAYS be in your archival memory, so remember to keep searching if you can’t find the answer. Answer the questions as if though the year is 2018.**", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p12:b0008", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 12, "block_id": "p12:b0008", "type": "text", "text": "---", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p12:b0009", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 12, "block_id": "p12:b0009", "type": "text", "text": "> **Questions were provided to MemGPT with the following prompt:**\n>\n> Search your archival memory to answer the provided question. Provide both the answer and the archival memory result from which you determined your answer. Format your response with the format `ANSWER: [YOUR ANSWER], DOCUMENT: [ARCHIVAL MEMORY TEXT]`. Your task is to answer the question:", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p12:b0010", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 12, "block_id": "p12:b0010", "type": "text", "text": "---", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p12:b0011", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 12, "block_id": "p12:b0011", "type": "text", "text": "> **For baselines, the following prompt along with a retrieved list of documents was provided:**\n>\n> Answer the question provided according to the list of documents below (some of which might be irrelevant. In your response, provide both the answer and the document text from which you determined the answer. Format your response with the format `ANSWER: <YOUR ANSWER>, DOCUMENT: [DOCUMENT TEXT]`. If none of the documents provided have the answer to the question, reply with `INSUFFICIENT INFORMATION`. Do NOT provide an answer if you cannot find it in the provided documents. Your response will only be considered correct if you provide both the answer and relevant document text, or say `INSUFFICIENT INFORMATION`. Answer the question as if though the current year is 2018.", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p12:b0012", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 12, "block_id": "p12:b0012", "type": "text", "text": "---", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p12:b0013", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 12, "block_id": "p12:b0013", "type": "text", "text": "## 6.1.5. LLM JUDGE (DOCUMENT ANALYSIS)", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p12:b0014", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 12, "block_id": "p12:b0014", "type": "text", "text": "In order to both check the correctness of the answer for the document analysis task, and also to ensure that the answer was properly derived from the provided text (rather than from the model weights), we used an LLM judge. The LLM judge was provided the answers generated by both baseline approaches and MemGPT, and asked to make a judgement with the following prompt:", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p12:b0015", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 12, "block_id": "p12:b0015", "type": "text", "text": "> Your task is to evaluate whether an LLM correct answered a question. The LLM response should be the format `\"ANSWER: [answer], DOCUMENT: [document.text]\"` or say `\"INSUFFICIENT INFORMATION\"`. The true answer is provided in the format `\"TRUE ANSWER:[list of possible`", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
{"id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0:p12:b0016", "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0", "page_number": 12, "block_id": "p12:b0016", "type": "text", "text": "---\n**Page 12**", "asset_path": null, "bbox_px": null, "crop_work_size": null, "span_id": null, "source": "ocr_md_rule"}
