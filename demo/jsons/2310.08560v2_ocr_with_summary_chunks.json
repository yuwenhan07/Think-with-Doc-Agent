{
  "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0",
  "source": {
    "type": "local_file",
    "path": "/home/work/workspace/Think-with-Doc-Agent/demo/doc/2310.08560v2.pdf"
  },
  "num_pages": 13,
  "metadata": {
    "doc_name": "2310.08560v2.pdf",
    "parser": {
      "renderer": "pymupdf",
      "dpi": 144
    }
  },
  "pages": [
    {
      "page_number": 1,
      "dpi": 144,
      "width_px": 1224,
      "height_px": 1584,
      "image_path": "/home/work/workspace/Think-with-Doc-Agent/demo/imgs/2310.08560v2/page_0001.png",
      "image_sha256": "sha256:4291004a6a3f394f8d7717d45a6930ee6634e61395569616963bf8fa745d94d7",
      "renderer": "pymupdf",
      "colorspace": "rgb",
      "text_raw": "# MemGPT: Towards LLMs as Operating Systems\n\n**Charles Packer**¬π **Sarah Wooders**¬π **Kevin Lin**¬π  \n**Vivian Fang**¬π **Shishir G. Patil**¬π **Ion Stoica**¬π **Joseph E. Gonzalez**¬π  \n\n---\n\n## Abstract\n\nLarge language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose *virtual context management*, a technique drawing inspiration from hierarchical memory systems in traditional operating systems which provide the illusion of an extended virtual memory via paging between physical memory and disk. Using this technique, we introduce **MemGPT** (MemoryGPT), a system that intelligently manages different storage tiers in order to effectively provide extended context within the LLM‚Äôs limited context window. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM‚Äôs context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at [https://research.memgpt.ai](https://research.memgpt.ai).\n\n---\n\n## 1. Introduction\n\nIn recent years, large language models (LLMs) and their underlying transformer architecture (Vaswani et al., 2017; Devlin et al., 2018; Brown et al., 2020; Ouyang et al., 2022) have become the cornerstone of conversational AI and have led to a wide array of consumer and enterprise applications. Despite these advances, the limited fixed-length context windows used by LLMs significantly hinders their applicability to long conversations or reasoning about long documents. For example, the most widely used open-source LLMs can only support a few dozen back-and-forth messages or reason about a short document before exceeding their maximum input length (Touvron et al., 2023).\n\nDirectly extending the context length of transformers incurs a quadratic increase in computational time and memory cost due to the transformer architecture‚Äôs self-attention mechanism, making the design of new long-context architectures a pressing research challenge (Dai et al., 2019; Kitaev et al., 2020; Beltagy et al., 2020). While developing longer models is an active area of research (Dong et al., 2023), even if we could overcome the computational challenges of context scaling, recent research shows that long-context models struggle to utilize additional context effectively (Liu et al., 2023a). As consequence, given the considerable resources needed to train state-of-the-art LLMs and diminishing returns of context scaling, there is a critical need for alternative techniques to support long context.\n\nIn this paper, we study how to provide the illusion of an infinite context while continuing to use fixed-context models. Our approach borrows from the idea of virtual memory paging that was developed to enable applications to work on datasets that far exceed the available memory by paging data between main memory and disk. We leverage the recent progress in function calling abilities of LLM agents (Schick et al., 2023; Liu et al., 2023b) to design **MemGPT**, an OS-inspired LLM system for *virtual context management*. Using function calls, LLM agents can read and write to external data sources, modify their own context, and choose when to return responses to the user.\n\nThese capabilities allow LLMs to effectively ‚Äúpage‚Äù in and out information between context windows (analogous to ‚Äúmain memory‚Äù in operating systems) and external storage, similar to hierarchical memory in traditional OSes. In addition, function calls can be leveraged to manage control flow between context management, response generation, and user interactions. This allows for an agent to choose to *iteratively* modify what is in its context for a single task, thereby more effectively utilizing its limited context.\n\nIn MemGPT, we treat context windows as a constrained memory resource, and design a memory hierarchy for LLMs analogous to memory tiers used in traditional OSes (Patterson et al., 1988). Applications in traditional OSes interact\n\n---\n\n¬πUniversity of California, Berkeley. Correspondence to: Charles Packer <cpacker@berkeley.edu>.\n\narXiv:2310.08560v2 [cs.AI] 12 Feb 2024",
      "text_source": "ocr",
      "spans": [],
      "diagnostics": {
        "ocr": {
          "model": "Qwen3-VL-32B-Instruct",
          "prompt": "qwenvl markdown",
          "min_pixels": 524288,
          "max_pixels": 4718592,
          "smart_resize": {
            "in_w": 1224,
            "in_h": 1584,
            "out_w": 1216,
            "out_h": 1600,
            "factor": 32
          },
          "elapsed_ms": 17558,
          "image_sha256": "sha256:4291004a6a3f394f8d7717d45a6930ee6634e61395569616963bf8fa745d94d7"
        }
      },
      "page_summary": "- The paper introduces **MemGPT**, an OS-inspired system for large language models (LLMs) that enables extended context handling by treating the LLM‚Äôs context window as a constrained memory resource, analogous to physical memory in traditional operating systems.\n- It proposes **virtual context management**, a technique inspired by virtual memory paging, which allows LLMs to ‚Äúpage‚Äù information in and out between their limited context window (main memory) and external storage (disk), thereby creating the illusion of infinite context.\n- The approach leverages recent advances in LLM function calling to enable agents to read/write external data, modify their context, and control response timing, allowing iterative refinement of context for better task performance.\n- MemGPT is evaluated in two domains: **document analysis** (handling documents larger than the LLM‚Äôs context window) and **multi-session chat** (creating conversational agents that remember, reflect, and evolve over time).\n- The authors argue that directly scaling context length is computationally expensive and often ineffective, making alternative techniques like virtual context management critical for practical long-context applications.\n- The paper releases code and data at [https://research.memgpt.ai](https://research.memgpt.ai) and is authored by researchers from the University of California, Berkeley.",
      "blocks": [
        {
          "block_id": "p1:b0001",
          "page_number": 1,
          "type": "text",
          "text": "# MemGPT: Towards LLMs as Operating Systems",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p1:b0002",
          "page_number": 1,
          "type": "text",
          "text": "**Charles Packer**¬π **Sarah Wooders**¬π **Kevin Lin**¬π  \n**Vivian Fang**¬π **Shishir G. Patil**¬π **Ion Stoica**¬π **Joseph E. Gonzalez**¬π",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p1:b0003",
          "page_number": 1,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p1:b0004",
          "page_number": 1,
          "type": "text",
          "text": "## Abstract",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p1:b0005",
          "page_number": 1,
          "type": "text",
          "text": "Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose *virtual context management*, a technique drawing inspiration from hierarchical memory systems in traditional operating systems which provide the illusion of an extended virtual memory via paging between physical memory and disk. Using this technique, we introduce **MemGPT** (MemoryGPT), a system that intelligently manages different storage tiers in order to effectively provide extended context within the LLM‚Äôs limited context window. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM‚Äôs context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users.\nWe release MemGPT code and data for our experiments at [https://research.memgpt.ai](https://research.memgpt.ai).",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p1:b0006",
          "page_number": 1,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p1:b0007",
          "page_number": 1,
          "type": "text",
          "text": "## 1. Introduction",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p1:b0008",
          "page_number": 1,
          "type": "text",
          "text": "In recent years, large language models (LLMs) and their underlying transformer architecture (Vaswani et al., 2017; Devlin et al., 2018; Brown et al., 2020; Ouyang et al., 2022) have become the cornerstone of conversational AI and have led to a wide array of consumer and enterprise applications. Despite these advances, the limited fixed-length context windows used by LLMs significantly hinders their applicability to long conversations or reasoning about long documents. For example, the most widely used open-source LLMs can only support a few dozen back-and-forth messages or reason about a short document before exceeding their maximum input length (Touvron et al., 2023).",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p1:b0009",
          "page_number": 1,
          "type": "text",
          "text": "Directly extending the context length of transformers incurs a quadratic increase in computational time and memory cost due to the transformer architecture‚Äôs self-attention mechanism, making the design of new long-context architectures a pressing research challenge (Dai et al., 2019; Kitaev et al., 2020; Beltagy et al., 2020). While developing longer models is an active area of research (Dong et al., 2023), even if we could overcome the computational challenges of context scaling, recent research shows that long-context models struggle to utilize additional context effectively (Liu et al., 2023a). As consequence, given the considerable resources needed to train state-of-the-art LLMs and diminishing returns of context scaling, there is a critical need for alternative techniques to support long context.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p1:b0010",
          "page_number": 1,
          "type": "text",
          "text": "In this paper, we study how to provide the illusion of an infinite context while continuing to use fixed-context models. Our approach borrows from the idea of virtual memory paging that was developed to enable applications to work on datasets that far exceed the available memory by paging data between main memory and disk. We leverage the recent progress in function calling abilities of LLM agents (Schick et al., 2023; Liu et al., 2023b) to design **MemGPT**, an OS-inspired LLM system for *virtual context management*. Using function calls, LLM agents can read and write to external data sources, modify their own context, and choose when to return responses to the user.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p1:b0011",
          "page_number": 1,
          "type": "text",
          "text": "These capabilities allow LLMs to effectively ‚Äúpage‚Äù in and out information between context windows (analogous to ‚Äúmain memory‚Äù in operating systems) and external storage, similar to hierarchical memory in traditional OSes. In addition, function calls can be leveraged to manage control flow between context management, response generation, and user interactions. This allows for an agent to choose to *iteratively* modify what is in its context for a single task, thereby more effectively utilizing its limited context.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p1:b0012",
          "page_number": 1,
          "type": "text",
          "text": "In MemGPT, we treat context windows as a constrained memory resource, and design a memory hierarchy for LLMs analogous to memory tiers used in traditional OSes (Patterson et al., 1988). Applications in traditional OSes interact",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p1:b0013",
          "page_number": 1,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p1:b0014",
          "page_number": 1,
          "type": "text",
          "text": "¬πUniversity of California, Berkeley. Correspondence to: Charles Packer <cpacker@berkeley.edu>.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p1:b0015",
          "page_number": 1,
          "type": "text",
          "text": "arXiv:2310.08560v2 [cs.AI] 12 Feb 2024",
          "source": "ocr_md_rule"
        }
      ]
    },
    {
      "page_number": 2,
      "dpi": 144,
      "width_px": 1224,
      "height_px": 1584,
      "image_path": "/home/work/workspace/Think-with-Doc-Agent/demo/imgs/2310.08560v2/page_0002.png",
      "image_sha256": "sha256:81169f03539386c246a8f4f55f173309081b6009814f76236702520797d2503e",
      "renderer": "pymupdf",
      "colorspace": "rgb",
      "text_raw": "# MemGPT: Towards LLMs as Operating Systems\n\n## Figure 1\n**February 7**\n\n> **User:** How was your day today?  \n> **MemGPT:** fun my bf james baked me a birthday cake  \n> **User:** Oh wow, happy birthday! üéâ  \n> **System Alert: Memory Pressure**  \n> `working_context.append(\"Birthday is February 7\")`  \n> `working_context.append(\"Boyfriend named James\")`\n\n*Figure 1. MemGPT (left) writes data to persistent memory after it receives a system alert about limited context space.*\n\n## Figure 2\n**February 7**\n\n> **User:** Did you do anything else to celebrate your birthday? üòä  \n> **MemGPT:** yeah we went to six flags!  \n> `recall_storage.search(\"six flags\")`  \n> *Showing 3 of 3 results (page 1/1):*  \n> `[01/24/2024] \"lol yeah six flags\"`  \n> `[01/14/2024] \"i love six flags been like 100 times\"`  \n> `[10/12/2023] \"james and I actually first met at six flags\"`  \n> **User:** Did you go with James? It's so cute how both met there!\n\n*Figure 2. MemGPT (left) can search out-of-context data to bring relevant information into the current context window.*\n\n---\n\nwith **virtual memory**, which provides an illusion of there being more memory resources than are actually available in physical (i.e., main) memory by the OS paging overflow data to disk and retrieving data (via a page fault) back into memory when accessed by applications. To provide a similar illusion of longer context length (analogous to virtual memory), we allow the LLM to manage what is placed in its own context (analogous to physical memory) via an ‚ÄòLLM OS‚Äô, which we call MemGPT. MemGPT enables the LLM to retrieve relevant historical data missing from what is placed in-context, and also evict less relevant data from context and into external storage systems. Figure 3 illustrates the components of MemGPT.\n\nThe combined use of a memory-hierarchy, OS functions and event-based control flow allow MemGPT to handle unbounded context using LLMs that have finite context windows. To demonstrate the utility of our new OS-inspired LLM system, we evaluate MemGPT on two domains where the performance of existing LLMs is severely limited by finite context: document analysis, where the length of standard text files can quickly exceed the input capacity of modern LLMs, and conversational agents, where LLMs bound by limited conversation windows lack context awareness, persona consistency, and long-term memory during extended conversations. In both settings, MemGPT is able to overcome the limitations of finite context to outperform existing LLM-based approaches.\n\n## 2. MemGPT (MemoryGPT)\n\nMemGPT‚Äôs OS-inspired multi-level memory architecture delineates between two primary memory types: **main context** (analogous to main memory/physical memory/RAM) and **external context** (analogous to disk memory/disk storage). Main context consists of the LLM prompt tokens‚Äîanything in main context is considered *in-context* and can be accessed by the LLM processor during inference. External context refers to any information that is held outside of the LLMs fixed context window. This *out-of-context* data must always be explicitly moved into main context in order for it to be passed to the LLM processor during inference. MemGPT provides function calls that the LLM processor to manage its own memory without any user intervention.\n\n### 2.1. Main context (prompt tokens)\n\nThe prompt tokens in MemGPT are split into three contiguous sections: the **system instructions**, **working context**, and **FIFO Queue**. The system instructions are read-only (static) and contain information on the MemGPT control flow, the intended usage of the different memory levels, and instructions on how to use the MemGPT functions (e.g. how to retrieve out-of-context data). Working context is a fixed-size read/write block of unstructured text, writable only via MemGPT function calls. In conversational settings, working context is intended to be used to store key facts, preferences, and other important information about the user and the persona the agent is adopting, allowing the agent to converse fluently with the user. The FIFO queue stores a rolling history of messages, including messages between the agent and user, as well as system messages (e.g. memory warnings) and function call inputs and outputs. The first index in the FIFO queue stores a system message containing a recursive summary of messages that have been evicted from the queue.\n\n### 2.2. Queue Manager\n\nThe queue manager manages messages in **recall storage** and the **FIFO queue**. When a new message is received by the system, the queue manager appends the incoming messages to the FIFO queue, concatenates the prompt tokens and triggers the LLM inference to generate LLM output (the completion tokens). The queue manager writes both the incoming message and the generated LLM output to recall storage (the MemGPT message database). When messages in recall storage are retrieved via a MemGPT function call, the queue manager appends them to the back of",
      "text_source": "ocr",
      "spans": [],
      "diagnostics": {
        "ocr": {
          "model": "Qwen3-VL-32B-Instruct",
          "prompt": "qwenvl markdown",
          "min_pixels": 524288,
          "max_pixels": 4718592,
          "smart_resize": {
            "in_w": 1224,
            "in_h": 1584,
            "out_w": 1216,
            "out_h": 1600,
            "factor": 32
          },
          "elapsed_ms": 19729,
          "image_sha256": "sha256:81169f03539386c246a8f4f55f173309081b6009814f76236702520797d2503e"
        }
      },
      "page_summary": "- **Page 2 introduces MemGPT**, an OS-inspired memory architecture for LLMs that mimics virtual memory by managing in-context and out-of-context data, enabling longer effective context windows through external storage and memory management functions.\n\n- **Figure 1** demonstrates MemGPT responding to a system alert (\"Memory Pressure\") by writing key facts (\"Birthday is February 7\", \"Boyfriend named James\") to persistent memory, illustrating proactive memory management.\n\n- **Figure 2** shows MemGPT retrieving out-of-context data via a `recall_storage.search(\"six flags\")` function call, bringing relevant historical information (e.g., past visits, meeting James there) into the current context to enrich the conversation.\n\n- **Section 2.1 (Main context)** defines the prompt tokens as three parts: **system instructions** (static, control flow), **working context** (read/write, for key user/persona facts), and **FIFO Queue** (rolling message history with a recursive summary at index 0).\n\n- **Section 2.2 (Queue Manager)** explains that it manages the FIFO queue and recall storage, appending new messages and LLM outputs to recall storage, and retrieving messages from recall storage when called, appending them to the queue for context.\n\n- The page emphasizes that MemGPT overcomes finite context limitations in LLMs for domains like document analysis and conversational agents by enabling context-aware, long-term memory and persona consistency through its memory hierarchy.",
      "blocks": [
        {
          "block_id": "p2:b0001",
          "page_number": 2,
          "type": "text",
          "text": "# MemGPT: Towards LLMs as Operating Systems",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0002",
          "page_number": 2,
          "type": "text",
          "text": "## Figure 1\n**February 7**",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0003",
          "page_number": 2,
          "type": "text",
          "text": "> **User:** How was your day today?  \n> **MemGPT:** fun my bf james baked me a birthday cake  \n> **User:** Oh wow, happy birthday! üéâ  \n> **System Alert: Memory Pressure**  \n> `working_context.append(\"Birthday is February 7\")`  \n> `working_context.append(\"Boyfriend named James\")`",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0004",
          "page_number": 2,
          "type": "text",
          "text": "*Figure 1. MemGPT (left) writes data to persistent memory after it receives a system alert about limited context space.*",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0005",
          "page_number": 2,
          "type": "text",
          "text": "## Figure 2\n**February 7**",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0006",
          "page_number": 2,
          "type": "text",
          "text": "> **User:** Did you do anything else to celebrate your birthday? üòä  \n> **MemGPT:** yeah we went to six flags!  \n> `recall_storage.search(\"six flags\")`  \n> *Showing 3 of 3 results (page 1/1):*  \n> `[01/24/2024] \"lol yeah six flags\"`  \n> `[01/14/2024] \"i love six flags been like 100 times\"`  \n> `[10/12/2023] \"james and I actually first met at six flags\"`  \n> **User:** Did you go with James? It's so cute how both met there!",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0007",
          "page_number": 2,
          "type": "text",
          "text": "*Figure 2. MemGPT (left) can search out-of-context data to bring relevant information into the current context window.*",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0008",
          "page_number": 2,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0009",
          "page_number": 2,
          "type": "text",
          "text": "with **virtual memory**, which provides an illusion of there being more memory resources than are actually available in physical (i.e., main) memory by the OS paging overflow data to disk and retrieving data (via a page fault) back into memory when accessed by applications. To provide a similar illusion of longer context length (analogous to virtual memory), we allow the LLM to manage what is placed in its own context (analogous to physical memory) via an ‚ÄòLLM OS‚Äô, which we call MemGPT. MemGPT enables the LLM to retrieve relevant historical data missing from what is placed in-context, and also evict less relevant data from context and into external storage systems. Figure 3 illustrates the components of MemGPT.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0010",
          "page_number": 2,
          "type": "text",
          "text": "The combined use of a memory-hierarchy, OS functions and event-based control flow allow MemGPT to handle unbounded context using LLMs that have finite context windows. To demonstrate the utility of our new OS-inspired LLM system, we evaluate MemGPT on two domains where the performance of existing LLMs is severely limited by finite context: document analysis, where the length of standard text files can quickly exceed the input capacity of modern LLMs, and conversational agents, where LLMs bound by limited conversation windows lack context awareness, persona consistency, and long-term memory during extended conversations. In both settings, MemGPT is able to overcome the limitations of finite context to outperform existing LLM-based approaches.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0011",
          "page_number": 2,
          "type": "text",
          "text": "## 2. MemGPT (MemoryGPT)",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0012",
          "page_number": 2,
          "type": "text",
          "text": "MemGPT‚Äôs OS-inspired multi-level memory architecture delineates between two primary memory types: **main context** (analogous to main memory/physical memory/RAM) and **external context** (analogous to disk memory/disk storage). Main context consists of the LLM prompt tokens‚Äîanything in main context is considered *in-context* and can be accessed by the LLM processor during inference. External context refers to any information that is held outside of the LLMs fixed context window. This *out-of-context* data must always be explicitly moved into main context in order for it to be passed to the LLM processor during inference. MemGPT provides function calls that the LLM processor to manage its own memory without any user intervention.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0013",
          "page_number": 2,
          "type": "text",
          "text": "### 2.1. Main context (prompt tokens)",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0014",
          "page_number": 2,
          "type": "text",
          "text": "The prompt tokens in MemGPT are split into three contiguous sections: the **system instructions**, **working context**, and **FIFO Queue**. The system instructions are read-only (static) and contain information on the MemGPT control flow, the intended usage of the different memory levels, and instructions on how to use the MemGPT functions (e.g. how to retrieve out-of-context data). Working context is a fixed-size read/write block of unstructured text, writable only via MemGPT function calls. In conversational settings, working context is intended to be used to store key facts, preferences, and other important information about the user and the persona the agent is adopting, allowing the agent to converse fluently with the user. The FIFO queue stores a rolling history of messages, including messages between the agent and user, as well as system messages (e.g. memory warnings) and function call inputs and outputs. The first index in the FIFO queue stores a system message containing a recursive summary of messages that have been evicted from the queue.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0015",
          "page_number": 2,
          "type": "text",
          "text": "### 2.2. Queue Manager",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0016",
          "page_number": 2,
          "type": "text",
          "text": "The queue manager manages messages in **recall storage** and the **FIFO queue**. When a new message is received by the system, the queue manager appends the incoming messages to the FIFO queue, concatenates the prompt tokens and triggers the LLM inference to generate LLM output (the completion tokens). The queue manager writes both the incoming message and the generated LLM output to recall storage (the MemGPT message database). When messages in recall storage are retrieved via a MemGPT function call, the queue manager appends them to the back of",
          "source": "ocr_md_rule"
        }
      ]
    },
    {
      "page_number": 3,
      "dpi": 144,
      "width_px": 1224,
      "height_px": 1584,
      "image_path": "/home/work/workspace/Think-with-Doc-Agent/demo/imgs/2310.08560v2/page_0003.png",
      "image_sha256": "sha256:ed8627f08638a8373fe8af7dd4eba52c95f6e7787f39439de40093479d02d2d6",
      "renderer": "pymupdf",
      "colorspace": "rgb",
      "text_raw": "# MemGPT: Towards LLMs as Operating Systems\n\n## Figure 3: MemGPT Architecture\n\n```plaintext\nLLM Finite Context Window (e.g. 8k tokens)\n--------------------------------------------\n| Prompt Tokens                          | Completion Tokens\n|----------------------------------------|-------------------\n| [System Instructions] (Read-Only)     | [Output Buffer]\n| (MemGPT System Prompt)                | (Read-Write)\n|                                        |\n| [Working Context] (Read-Write)        | \n| (Write via Functions)                 |\n|                                        |\n| [FIFO Queue] (Read-Write)             |\n| (Write via Queue Manager)             |\n|                                        |\n| [Function Executor]                   |\n| (Read via Functions, Write via Functions) |\n|                                        |\n| [Queue Manager]                       |\n| (Read via Functions, Write via Queue Manager) |\n|                                        |\n| [Archival Storage]                    | [Recall Storage]\n| (Read via Functions, Write via Functions) | (Read via Functions, Write via Queue Manager)\n```\n\n**Figure 3.** In MemGPT, a fixed-context LLM processor is augmented with a hierarchical memory system and functions that let it manage its own memory. The LLM‚Äôs prompt tokens (inputs), or *main context*, consist of the system instructions, working context, and a FIFO queue. The LLM completion tokens (outputs) are interpreted as function calls by the function executor. MemGPT uses functions to move data between main context and *external context* (the archival and recall storage databases). The LLM can request immediate follow-up LLM inference to chain function calls together by generating a special keyword argument (`request_heartbeat=true`) in its output; function chaining is what allows MemGPT to perform multi-step retrieval to answer user queries.\n\n---\n\n## Memory Management and Queue Eviction\n\nThe queue manager is also responsible for controlling context overflow via a queue eviction policy. When the prompt tokens exceed the ‚Äòwarning token count‚Äô of the underlying LLM‚Äôs context window (e.g. 70% of the context window), the queue manager inserts a system message into the queue warning the LLM of an impending queue eviction (a ‚Äòmemory pressure‚Äô warning) to allow the LLM to use MemGPT functions to store important information contained in the FIFO queue to working context or **archival storage** (a read/write database storing arbitrary length text objects). When the prompt tokens exceed the ‚Äòflush token count‚Äô (e.g. 100% of the context window), the queue manager flushes the queue to free up space in the context window: the queue manager evicts a specific count of messages (e.g. 50% of the context window), generates a new recursive summary using the existing recursive summary and evicted messages. Once the queue is flushed, the evicted messages are no longer in-context and immediately viewable to the LLM, however they are stored indefinitely in recall storage and readable via MemGPT function calls.\n\n---\n\n## 2.3. Function Executor (Handling of Completion Tokens)\n\nMemGPT orchestrates data movement between main context and external context via function calls that are generated by the LLM processor. Memory edits and retrieval are entirely self-directed: MemGPT autonomously updates and searches through its own memory based on the current context. For instance, it can decide when to move items between contexts (e.g. when the conversation history is becoming too long, as shown in Figure 1) and modify its main context to better reflect its evolving understanding of its current objectives and responsibilities (as shown in Figure 3). We implement self-directed editing and retrieval by providing explicit instructions within the system instructions that guide the LLM on how to interact with the MemGPT memory systems. These instructions comprise two main components: (1) a detailed description of the memory hierarchy and their respective utilities, and (2) a function schema (complete with their natural language descriptions) that the system can call to access or modify its memory.\n\nDuring each inference cycle, the LLM processor takes main context (concatenated into a single string) as input, and generates an output string. This output string is parsed by MemGPT to ensure correctness, and if the parser validates the function arguments, the function is executed. The results, including any runtime errors that occur (e.g. trying to add to main context when it is already at maximum capacity), are then fed back to the processor by MemGPT. This feedback loop enables the system to learn from its actions and adjust its behavior accordingly. Awareness of context limits is a key aspect in making the self-editing mechanism work effectively; to this end, MemGPT prompts the processor with warnings regarding token limitations to guide its memory management decisions. Additionally, our memory retrieval mechanisms are designed to be cognizant of these token constraints and implement pagination to prevent retrieval calls from overflowing the context window.",
      "text_source": "ocr",
      "spans": [],
      "diagnostics": {
        "ocr": {
          "model": "Qwen3-VL-32B-Instruct",
          "prompt": "qwenvl markdown",
          "min_pixels": 524288,
          "max_pixels": 4718592,
          "smart_resize": {
            "in_w": 1224,
            "in_h": 1584,
            "out_w": 1216,
            "out_h": 1600,
            "factor": 32
          },
          "elapsed_ms": 17565,
          "image_sha256": "sha256:ed8627f08638a8373fe8af7dd4eba52c95f6e7787f39439de40093479d02d2d6"
        }
      },
      "page_summary": "**Summary of Page 3 (MemGPT: Towards LLMs as Operating Systems):**\n\n- **Figure 3** illustrates MemGPT‚Äôs architecture, where a fixed-context LLM is augmented with a hierarchical memory system (system instructions, working context, FIFO queue, archival storage, recall storage) and function-based memory management. The LLM‚Äôs prompt tokens (main context) feed into the model, while completion tokens are parsed as function calls by the Function Executor.\n\n- The **queue manager** controls context overflow via a two-tier eviction policy: a ‚Äúmemory pressure‚Äù warning at 70% context usage prompts the LLM to store data in working context or archival storage; at 100% usage, it flushes the queue by evicting messages (e.g., 50% of context), generating a recursive summary, and storing evicted content in recall storage for later retrieval.\n\n- The **function executor** handles completion tokens by interpreting them as function calls to move data between main and external contexts. Memory edits and retrievals are self-directed, guided by system instructions that include a memory hierarchy description and a function schema with natural language descriptions.\n\n- During inference, the LLM processes the main context, generates an output string, which is parsed and validated by MemGPT. Valid functions are executed, and results (including errors) are fed back to the LLM, creating a feedback loop for adaptive behavior.\n\n- MemGPT proactively manages token limits by issuing warnings and using pagination in retrieval to prevent context overflow, ensuring the system remains within its finite context window while enabling long-term memory access.\n\n- The system enables multi-step retrieval through function chaining, triggered by the LLM generating `request_heartbeat=true` in its output, allowing it to perform complex, iterative tasks.",
      "blocks": [
        {
          "block_id": "p3:b0001",
          "page_number": 3,
          "type": "text",
          "text": "# MemGPT: Towards LLMs as Operating Systems",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p3:b0002",
          "page_number": 3,
          "type": "text",
          "text": "## Figure 3: MemGPT Architecture",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p3:b0003",
          "page_number": 3,
          "type": "text",
          "text": "```plaintext\nLLM Finite Context Window (e.g. 8k tokens)\n--------------------------------------------\n| Prompt Tokens                          | Completion Tokens\n|----------------------------------------|-------------------\n| [System Instructions] (Read-Only)     | [Output Buffer]\n| (MemGPT System Prompt)                | (Read-Write)\n|                                        |\n| [Working Context] (Read-Write)        | \n| (Write via Functions)                 |\n|                                        |\n| [FIFO Queue] (Read-Write)             |\n| (Write via Queue Manager)             |\n|                                        |\n| [Function Executor]                   |\n| (Read via Functions, Write via Functions) |\n|                                        |\n| [Queue Manager]                       |\n| (Read via Functions, Write via Queue Manager) |\n|                                        |\n| [Archival Storage]                    | [Recall Storage]\n| (Read via Functions, Write via Functions) | (Read via Functions, Write via Queue Manager)\n```",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p3:b0004",
          "page_number": 3,
          "type": "text",
          "text": "**Figure 3.** In MemGPT, a fixed-context LLM processor is augmented with a hierarchical memory system and functions that let it manage its own memory. The LLM‚Äôs prompt tokens (inputs), or *main context*, consist of the system instructions, working context, and a FIFO queue. The LLM completion tokens (outputs) are interpreted as function calls by the function executor. MemGPT uses functions to move data between main context and *external context* (the archival and recall storage databases). The LLM can request immediate follow-up LLM inference to chain function calls together by generating a special keyword argument (`request_heartbeat=true`) in its output; function chaining is what allows MemGPT to perform multi-step retrieval to answer user queries.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p3:b0005",
          "page_number": 3,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p3:b0006",
          "page_number": 3,
          "type": "text",
          "text": "## Memory Management and Queue Eviction",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p3:b0007",
          "page_number": 3,
          "type": "text",
          "text": "The queue manager is also responsible for controlling context overflow via a queue eviction policy. When the prompt tokens exceed the ‚Äòwarning token count‚Äô of the underlying LLM‚Äôs context window (e.g. 70% of the context window), the queue manager inserts a system message into the queue warning the LLM of an impending queue eviction (a ‚Äòmemory pressure‚Äô warning) to allow the LLM to use MemGPT functions to store important information contained in the FIFO queue to working context or **archival storage** (a read/write database storing arbitrary length text objects). When the prompt tokens exceed the ‚Äòflush token count‚Äô (e.g. 100% of the context window), the queue manager flushes the queue to free up space in the context window: the queue manager evicts a specific count of messages (e.g. 50% of the context window), generates a new recursive summary using the existing recursive summary and evicted messages. Once the queue is flushed, the evicted messages are no longer in-context and immediately viewable to the LLM, however they are stored indefinitely in recall storage and readable via MemGPT function calls.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p3:b0008",
          "page_number": 3,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p3:b0009",
          "page_number": 3,
          "type": "text",
          "text": "## 2.3. Function Executor (Handling of Completion Tokens)",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p3:b0010",
          "page_number": 3,
          "type": "text",
          "text": "MemGPT orchestrates data movement between main context and external context via function calls that are generated by the LLM processor. Memory edits and retrieval are entirely self-directed: MemGPT autonomously updates and searches through its own memory based on the current context. For instance, it can decide when to move items between contexts (e.g. when the conversation history is becoming too long, as shown in Figure 1) and modify its main context to better reflect its evolving understanding of its current objectives and responsibilities (as shown in Figure 3). We implement self-directed editing and retrieval by providing explicit instructions within the system instructions that guide the LLM on how to interact with the MemGPT memory systems. These instructions comprise two main components: (1) a detailed description of the memory hierarchy and their respective utilities, and (2) a function schema (complete with their natural language descriptions) that the system can call to access or modify its memory.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p3:b0011",
          "page_number": 3,
          "type": "text",
          "text": "During each inference cycle, the LLM processor takes main context (concatenated into a single string) as input, and generates an output string. This output string is parsed by MemGPT to ensure correctness, and if the parser validates the function arguments, the function is executed. The results, including any runtime errors that occur (e.g. trying to add to main context when it is already at maximum capacity), are then fed back to the processor by MemGPT. This feedback loop enables the system to learn from its actions and adjust its behavior accordingly. Awareness of context limits is a key aspect in making the self-editing mechanism work effectively; to this end, MemGPT prompts the processor with warnings regarding token limitations to guide its memory management decisions. Additionally, our memory retrieval mechanisms are designed to be cognizant of these token constraints and implement pagination to prevent retrieval calls from overflowing the context window.",
          "source": "ocr_md_rule"
        }
      ]
    },
    {
      "page_number": 4,
      "dpi": 144,
      "width_px": 1224,
      "height_px": 1584,
      "image_path": "/home/work/workspace/Think-with-Doc-Agent/demo/imgs/2310.08560v2/page_0004.png",
      "image_sha256": "sha256:757385a9be16266292daf6b6c3a1cc4a791b90f252931cbb8d25550a6b569cfa",
      "renderer": "pymupdf",
      "colorspace": "rgb",
      "text_raw": "# MemGPT: Towards LLMs as Operating Systems\n\n## Table 1. Comparing context lengths of commonly used models and LLM APIs (data collected 1/2024)\n\n*Approximate message count assuming a preprompt of 1k tokens, and an average message size of ~50 tokens (~250 characters). 'Open' means the model is open-source or open-weights (vs only available behind an API).*\n\n| Model / API name          | Open? | Context Window Tokens | *Messages |\n|---------------------------|-------|------------------------|-----------|\n| Llama (1)                 | ‚úì     | 2k                     | 20        |\n| Llama 2                   | ‚úì     | 4k                     | 60        |\n| GPT-3.5 Turbo (release)   | ‚úó     | 4k                     | 60        |\n| Mistral 7B                | ‚úì     | 8k                     | 140       |\n| GPT-4 (release)           | ‚úó     | 8k                     | 140       |\n| GPT-3.5 Turbo             | ‚úó     | 16k                    | 300       |\n| GPT-4                     | ‚úó     | 32k                    | ~600      |\n| Claude 2                  | ‚úó     | 100k                   | ~2000     |\n| GPT-4 Turbo               | ‚úó     | 128k                   | ~2600     |\n| Yi-34B-200k               | ‚úì     | 200k                   | ~4000     |\n\n---\n\n## Figure 4. An example conversation snippet where MemGPT (left) updates stored information.\n\nHere the information is stored in working context memory (located within the prompt tokens).\n\n> **User**: How's James doing? Any special plans today?  \n> **MemGPT**: `working_context.replace(\"Boyfriend named James\", \"Ex-boyfriend named James\")`  \n> **User**: actually james and i broke up  \n> **MemGPT**: Sorry to hear that - hope you're OK üíî\n\n---\n\n## 2.4. Control flow and function chaining\n\nIn MemGPT, *events* trigger LLM inference: events are generalized inputs to MemGPT and can consist of user messages (in chat applications), system messages (e.g., main context capacity warnings), user interactions (e.g., an alert that a user just logged in, or an alert that they finished uploading a document), and timed events that are run on a regular schedule (allowing MemGPT to run ‚Äòunprompted‚Äô without user intervention). MemGPT processes events with a parser to convert them into plain text messages that can be appended to main context and eventually be fed as input into the LLM processor.\n\nMany practical tasks require calling multiple functions in sequence, for example, navigating through multiple pages of results from a single query or collating data from different documents in main context from separate queries. Function chaining allows MemGPT to execute multiple function calls sequentially before returning control to the user. In MemGPT, functions can be called with a special flag that requests control be immediately returned to the processor after the requested function completes execution. If this flag is present, MemGPT will add the function output to main context and (as opposed to pausing processor execution). If this flag is not present (a *yield*), MemGPT will not run the LLM processor until the next external event trigger (e.g., a user message or scheduled interrupt).\n\n---\n\n## 3. Experiments\n\nWe assess MemGPT in two long-context domains: conversational agents and document analysis. For conversational agents, we expand the existing Multi-Session Chat dataset (Xu et al., 2021) and introduce two new dialogue tasks that evaluate an agent‚Äôs ability to retain knowledge across long conversations. For document analysis, we benchmark MemGPT on existing tasks from (Liu et al., 2023a) for question answering and key-value retrieval over lengthy documents. We also propose a new nested key-value retrieval task requiring collating information across multiple data sources, which tests the ability of an agent to collate information from multiple data sources (multi-hop retrieval). We publicly release our augmented MSC dataset, nested KV retrieval dataset, and a dataset of embeddings for 20M Wikipedia articles to facilitate future research. Our code for the benchmarks is available at [https://research.memgpt.ai](https://research.memgpt.ai).\n\n### Implementation details\n\nWhen discussing OpenAI models, unless otherwise specified:\n- ‚ÄòGPT-4 Turbo‚Äô refers to the specific `gpt-4-1106-preview` model endpoint (context window of 128,000),\n- ‚ÄòGPT-4‚Äô refers to `gpt-4-0613` (context window of 8,192),\n- ‚ÄòGPT-3.5 Turbo‚Äô refers to `gpt-3.5-turbo-1106` (context window of 16,385).\n\nIn experiments, we run MemGPT with all baseline models (GPT-4, GPT-4 Turbo, and GPT 3.5) to show how the underlying model performance affects MemGPT‚Äôs.\n\n---\n\n## 3.1. MemGPT for conversational agents\n\nConversational agents like virtual companions and personalized assistants aim to engage users in natural, long-term interactions, potentially spanning weeks, months, or even years. This creates challenges for models with fixed-length contexts, which can only reference a limited history of the conversation. An ‚Äòinfinite context‚Äô agent should seamlessly handle continuous exchanges without boundary or reset.\n\nWhen conversing with a user, such an agent must satisfy two key criteria:\n1. **Consistency** - The agent should maintain conversational coherence. New facts, preferences, and events mentioned should align with prior statements from both the user and agent.\n2. **Engagement** - The agent should draw on long-term knowledge about the user to personalize interactions.\n\n---\n**Date**: February 14\n**Page**: 4",
      "text_source": "ocr",
      "spans": [],
      "diagnostics": {
        "ocr": {
          "model": "Qwen3-VL-32B-Instruct",
          "prompt": "qwenvl markdown",
          "min_pixels": 524288,
          "max_pixels": 4718592,
          "smart_resize": {
            "in_w": 1224,
            "in_h": 1584,
            "out_w": 1216,
            "out_h": 1600,
            "factor": 32
          },
          "elapsed_ms": 22843,
          "image_sha256": "sha256:757385a9be16266292daf6b6c3a1cc4a791b90f252931cbb8d25550a6b569cfa"
        }
      },
      "page_summary": "- **Table 1** compares context lengths of major LLMs and APIs (as of Jan 2024), showing context window sizes in tokens and approximate message counts, with open-source models (e.g., Llama, Mistral, Yi-34B-200k) and proprietary models (e.g., GPT-4, Claude 2) listed.\n- **Figure 4** illustrates MemGPT‚Äôs ability to update stored information in real-time during a conversation, demonstrating how it modifies its working context memory (e.g., changing ‚ÄúBoyfriend named James‚Äù to ‚ÄúEx-boyfriend named James‚Äù) based on user input.\n- Section **2.4** describes MemGPT‚Äôs control flow and function chaining: events (user messages, system alerts, timed triggers) are parsed into plain text and fed to the LLM; function calls can be synchronous (immediate return) or asynchronous (yield, waiting for next event).\n- Section **3 (Experiments)** outlines evaluations in two domains: conversational agents (using an expanded Multi-Session Chat dataset) and document analysis (including a new nested key-value retrieval task for multi-hop retrieval), with datasets and code publicly released at https://research.memgpt.ai.\n- **Implementation details** clarify the specific OpenAI model endpoints used (e.g., GPT-4 Turbo = gpt-4-1106-preview, 128k context) and note that experiments use GPT-4, GPT-4 Turbo, and GPT-3.5 to assess model impact.\n- Section **3.1** defines the two key criteria for conversational agents: **Consistency** (maintaining coherent, aligned dialogue) and **Engagement** (personalizing interactions using long-term user knowledge).",
      "blocks": [
        {
          "block_id": "p4:b0001",
          "page_number": 4,
          "type": "text",
          "text": "# MemGPT: Towards LLMs as Operating Systems",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0002",
          "page_number": 4,
          "type": "text",
          "text": "## Table 1. Comparing context lengths of commonly used models and LLM APIs (data collected 1/2024)",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0003",
          "page_number": 4,
          "type": "text",
          "text": "*Approximate message count assuming a preprompt of 1k tokens, and an average message size of ~50 tokens (~250 characters). 'Open' means the model is open-source or open-weights (vs only available behind an API).*",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0004",
          "page_number": 4,
          "type": "text",
          "text": "| Model / API name          | Open? | Context Window Tokens | *Messages |\n|---------------------------|-------|------------------------|-----------|\n| Llama (1)                 | ‚úì     | 2k                     | 20        |\n| Llama 2                   | ‚úì     | 4k                     | 60        |\n| GPT-3.5 Turbo (release)   | ‚úó     | 4k                     | 60        |\n| Mistral 7B                | ‚úì     | 8k                     | 140       |\n| GPT-4 (release)           | ‚úó     | 8k                     | 140       |\n| GPT-3.5 Turbo             | ‚úó     | 16k                    | 300       |\n| GPT-4                     | ‚úó     | 32k                    | ~600      |\n| Claude 2                  | ‚úó     | 100k                   | ~2000     |\n| GPT-4 Turbo               | ‚úó     | 128k                   | ~2600     |\n| Yi-34B-200k               | ‚úì     | 200k                   | ~4000     |",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0005",
          "page_number": 4,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0006",
          "page_number": 4,
          "type": "text",
          "text": "## Figure 4. An example conversation snippet where MemGPT (left) updates stored information.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0007",
          "page_number": 4,
          "type": "text",
          "text": "Here the information is stored in working context memory (located within the prompt tokens).",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0008",
          "page_number": 4,
          "type": "text",
          "text": "> **User**: How's James doing? Any special plans today?  \n> **MemGPT**: `working_context.replace(\"Boyfriend named James\", \"Ex-boyfriend named James\")`  \n> **User**: actually james and i broke up  \n> **MemGPT**: Sorry to hear that - hope you're OK üíî",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0009",
          "page_number": 4,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0010",
          "page_number": 4,
          "type": "text",
          "text": "## 2.4. Control flow and function chaining",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0011",
          "page_number": 4,
          "type": "text",
          "text": "In MemGPT, *events* trigger LLM inference: events are generalized inputs to MemGPT and can consist of user messages (in chat applications), system messages (e.g., main context capacity warnings), user interactions (e.g., an alert that a user just logged in, or an alert that they finished uploading a document), and timed events that are run on a regular schedule (allowing MemGPT to run ‚Äòunprompted‚Äô without user intervention). MemGPT processes events with a parser to convert them into plain text messages that can be appended to main context and eventually be fed as input into the LLM processor.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0012",
          "page_number": 4,
          "type": "text",
          "text": "Many practical tasks require calling multiple functions in sequence, for example, navigating through multiple pages of results from a single query or collating data from different documents in main context from separate queries. Function chaining allows MemGPT to execute multiple function calls sequentially before returning control to the user. In MemGPT, functions can be called with a special flag that requests control be immediately returned to the processor after the requested function completes execution. If this flag is present, MemGPT will add the function output to main context and (as opposed to pausing processor execution). If this flag is not present (a *yield*), MemGPT will not run the LLM processor until the next external event trigger (e.g., a user message or scheduled interrupt).",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0013",
          "page_number": 4,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0014",
          "page_number": 4,
          "type": "text",
          "text": "## 3. Experiments",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0015",
          "page_number": 4,
          "type": "text",
          "text": "We assess MemGPT in two long-context domains: conversational agents and document analysis. For conversational agents, we expand the existing Multi-Session Chat dataset (Xu et al., 2021) and introduce two new dialogue tasks that evaluate an agent‚Äôs ability to retain knowledge across long conversations. For document analysis, we benchmark MemGPT on existing tasks from (Liu et al., 2023a) for question answering and key-value retrieval over lengthy documents. We also propose a new nested key-value retrieval task requiring collating information across multiple data sources, which tests the ability of an agent to collate information from multiple data sources (multi-hop retrieval). We publicly release our augmented MSC dataset, nested KV retrieval dataset, and a dataset of embeddings for 20M Wikipedia articles to facilitate future research. Our code for the benchmarks is available at [https://research.memgpt.ai](https://research.memgpt.ai).",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0016",
          "page_number": 4,
          "type": "text",
          "text": "### Implementation details",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0017",
          "page_number": 4,
          "type": "text",
          "text": "When discussing OpenAI models, unless otherwise specified:\n- ‚ÄòGPT-4 Turbo‚Äô refers to the specific `gpt-4-1106-preview` model endpoint (context window of 128,000),\n- ‚ÄòGPT-4‚Äô refers to `gpt-4-0613` (context window of 8,192),\n- ‚ÄòGPT-3.5 Turbo‚Äô refers to `gpt-3.5-turbo-1106` (context window of 16,385).",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0018",
          "page_number": 4,
          "type": "text",
          "text": "In experiments, we run MemGPT with all baseline models (GPT-4, GPT-4 Turbo, and GPT 3.5) to show how the underlying model performance affects MemGPT‚Äôs.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0019",
          "page_number": 4,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0020",
          "page_number": 4,
          "type": "text",
          "text": "## 3.1. MemGPT for conversational agents",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0021",
          "page_number": 4,
          "type": "text",
          "text": "Conversational agents like virtual companions and personalized assistants aim to engage users in natural, long-term interactions, potentially spanning weeks, months, or even years. This creates challenges for models with fixed-length contexts, which can only reference a limited history of the conversation. An ‚Äòinfinite context‚Äô agent should seamlessly handle continuous exchanges without boundary or reset.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0022",
          "page_number": 4,
          "type": "text",
          "text": "When conversing with a user, such an agent must satisfy two key criteria:\n1. **Consistency** - The agent should maintain conversational coherence. New facts, preferences, and events mentioned should align with prior statements from both the user and agent.\n2. **Engagement** - The agent should draw on long-term knowledge about the user to personalize interactions.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p4:b0023",
          "page_number": 4,
          "type": "text",
          "text": "---\n**Date**: February 14\n**Page**: 4",
          "source": "ocr_md_rule"
        }
      ]
    },
    {
      "page_number": 5,
      "dpi": 144,
      "width_px": 1224,
      "height_px": 1584,
      "image_path": "/home/work/workspace/Think-with-Doc-Agent/demo/imgs/2310.08560v2/page_0005.png",
      "image_sha256": "sha256:8cc746ffcf14c64351de7e62746331d4abd7ac2375e212767a3d7db51c9f32e1",
      "renderer": "pymupdf",
      "colorspace": "rgb",
      "text_raw": "# MemGPT: Towards LLMs as Operating Systems\n\n## Table 2. Deep memory retrieval (DMR) performance.\n\nIn this task, the agent is asked a specific question about a topic discussed in a prior conversation (sessions 1‚Äì5). The agent‚Äôs response is scored against the gold answer. MemGPT significantly outperforms the fixed-context baselines.\n\n| Model             | Accuracy ‚Üë | ROUGE-L (R) ‚Üë |\n|-------------------|------------|---------------|\n| GPT-3.5 Turbo     | 38.7%      | 0.394         |\n| + MemGPT          | 66.9%      | 0.629         |\n| GPT-4             | 32.1%      | 0.296         |\n| + MemGPT          | 92.5%      | 0.814         |\n| GPT-4 Turbo       | 35.3%      | 0.359         |\n| + **MemGPT**      | **93.4%**  | **0.827**     |\n\n## Table 3. Conversation opener performance.\n\nThe agent‚Äôs conversation opener is evaluated using similarity scores to the gold persona labels (SIM-1/3) and to the human-created opener (SIM-H). MemGPT is able to exceed the performance of the human-created conversation opener with a variety of underlying models.\n\n| Method          | ‚Üë SIM-1 | SIM-3 | SIM-H |\n|-----------------|---------|-------|-------|\n| Human           | 0.800   | 0.800 | 1.000 |\n| GPT-3.5 Turbo   | 0.830   | 0.812 | 0.817 |\n| GPT-4           | **0.868** | **0.843** | **0.773** |\n| GPT-4 Turbo     | 0.857   | 0.828 | 0.767 |\n\n---\n\nresponses. Referencing prior conversations makes dialogue more natural and engaging.\n\nWe therefore assess our proposed system, MemGPT, on these two criteria: (1) Does MemGPT leverage its memory to improve conversation consistency? Can it remember relevant facts, preferences, and events from past interactions to maintain coherence? (2) Does MemGPT produce more engaging dialogue by taking advantage of memory? Does it spontaneously incorporate long-range user information to personalize messages? By evaluating on consistency and engagement, we can determine how well MemGPT handles the challenges of long-term conversational interaction compared to fixed-context baselines. Its ability to satisfy these criteria will demonstrate whether unbounded context provides meaningful benefits for conversational agents.\n\n### Dataset\n\nWe evaluate MemGPT and our fixed-context baselines on the Multi-Session Chat (MSC) dataset introduced by Xu et al. (2021), which contains multi-session chat logs generated by human labelers, each of whom was asked to play a consistent persona for the duration of all sessions. Each multi-session chat in MSC has five total sessions, and each session consists of a roughly a dozen messages. As part of our consistency experiments, we created a new session (session 6) that contains a single question-answer response pair between the same two personas.\n\n### 3.1.1. Deep Memory Retrieval Task (Consistency)\n\nWe introduce a new ‚Äòdeep memory retrieval‚Äô (DMR) task based on the MSC dataset designed to test the consistency of a conversational agent. In DMR, the conversational agent is asked a question by the user that explicitly refers back to a prior conversation and has a very narrow expected answer range. We generated the DMR question-answer (QA) pairs using a separate LLM that was instructed to write a question from one user to another that could only be answered correctly using knowledge gained from the past sessions (see Appendix for further details).\n\nWe evaluate the quality of the generated response against the ‚Äògold response‚Äô using ROUGE-L scores (Lin, 2004) and an ‚ÄòLLM judge‚Äô, which is instructed to evaluate whether or not the generated response is consistent with the gold response (GPT-4 has been shown to have high agreement with human evaluators (Zheng et al., 2023)). In practice, we notice that the generated responses (from both MemGPT and the baselines) were generally more verbose than the gold responses. We use the ROUGE-L recall (R) metric to account for the verbosity of the generated agent replies compared to the relatively short gold answer labels.\n\n**MemGPT utilizes memory to maintain coherence:** Table 2 shows the performance of MemGPT vs the fixed-memory baselines. We compare MemGPT using different underlying LLMs, and compare against using the base LLM without MemGPT as a baseline. The baselines are able to see a lossy summarization of the past five conversations to mimic an extended recursive summarization procedure, while MemGPT instead has access to the full conversation history but must access it via paginated search queries to recall memory (in order to bring them into main context). In this task, we see that MemGPT clearly improves the performance of the underlying base LLM: there is a clear drop in both accuracy and ROUGE scores when going from MemGPT to the corresponding LLM baselines.\n\n### 3.1.2. Conversation Opener Task (Engagement)\n\nIn the ‚Äòconversation opener‚Äô task we evaluate an agent‚Äôs ability to craft engaging messages to the user that draw from knowledge accumulated in prior conversations. To evaluate the ‚Äòengagingness‚Äô of a conversation opener using the MSC dataset, we compare the generated opener to the gold personas: an engaging conversation opener should draw from one (or several) of the data points contained in the persona, which in MSC effectively summarize the knowledge accumulated throughout all prior sessions. We also compare to the human-generated gold opener, i.e., the",
      "text_source": "ocr",
      "spans": [],
      "diagnostics": {
        "ocr": {
          "model": "Qwen3-VL-32B-Instruct",
          "prompt": "qwenvl markdown",
          "min_pixels": 524288,
          "max_pixels": 4718592,
          "smart_resize": {
            "in_w": 1224,
            "in_h": 1584,
            "out_w": 1216,
            "out_h": 1600,
            "factor": 32
          },
          "elapsed_ms": 22073,
          "image_sha256": "sha256:8cc746ffcf14c64351de7e62746331d4abd7ac2375e212767a3d7db51c9f32e1"
        }
      },
      "page_summary": "- The page presents two key evaluation tasks for MemGPT: Deep Memory Retrieval (DMR) for consistency and Conversation Opener for engagement, using the Multi-Session Chat (MSC) dataset.\n- Table 2 (DMR task) shows MemGPT significantly outperforms fixed-context baselines across all models (GPT-3.5 Turbo, GPT-4, GPT-4 Turbo), with accuracy improvements from 38.7% to 66.9% and ROUGE-L scores from 0.394 to 0.827.\n- Table 3 (Conversation Opener task) demonstrates MemGPT‚Äôs ability to generate engaging openers, with GPT-4 + MemGPT achieving the highest SIM-1 (0.868) and SIM-3 (0.843) scores, and outperforming the human-created opener in SIM-1 and SIM-3, though slightly lower in SIM-H.\n- MemGPT maintains coherence by accessing full conversation history via paginated memory search, unlike baselines that rely on lossy summaries, leading to clear performance gains in consistency.\n- The conversation opener task evaluates engagement by comparing generated messages to gold personas and human-generated openers, showing MemGPT‚Äôs effectiveness in personalizing dialogue using long-range memory.",
      "blocks": [
        {
          "block_id": "p5:b0001",
          "page_number": 5,
          "type": "text",
          "text": "# MemGPT: Towards LLMs as Operating Systems",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p5:b0002",
          "page_number": 5,
          "type": "text",
          "text": "## Table 2. Deep memory retrieval (DMR) performance.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p5:b0003",
          "page_number": 5,
          "type": "text",
          "text": "In this task, the agent is asked a specific question about a topic discussed in a prior conversation (sessions 1‚Äì5). The agent‚Äôs response is scored against the gold answer. MemGPT significantly outperforms the fixed-context baselines.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p5:b0004",
          "page_number": 5,
          "type": "text",
          "text": "| Model             | Accuracy ‚Üë | ROUGE-L (R) ‚Üë |\n|-------------------|------------|---------------|\n| GPT-3.5 Turbo     | 38.7%      | 0.394         |\n| + MemGPT          | 66.9%      | 0.629         |\n| GPT-4             | 32.1%      | 0.296         |\n| + MemGPT          | 92.5%      | 0.814         |\n| GPT-4 Turbo       | 35.3%      | 0.359         |\n| + **MemGPT**      | **93.4%**  | **0.827**     |",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p5:b0005",
          "page_number": 5,
          "type": "text",
          "text": "## Table 3. Conversation opener performance.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p5:b0006",
          "page_number": 5,
          "type": "text",
          "text": "The agent‚Äôs conversation opener is evaluated using similarity scores to the gold persona labels (SIM-1/3) and to the human-created opener (SIM-H). MemGPT is able to exceed the performance of the human-created conversation opener with a variety of underlying models.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p5:b0007",
          "page_number": 5,
          "type": "text",
          "text": "| Method          | ‚Üë SIM-1 | SIM-3 | SIM-H |\n|-----------------|---------|-------|-------|\n| Human           | 0.800   | 0.800 | 1.000 |\n| GPT-3.5 Turbo   | 0.830   | 0.812 | 0.817 |\n| GPT-4           | **0.868** | **0.843** | **0.773** |\n| GPT-4 Turbo     | 0.857   | 0.828 | 0.767 |",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p5:b0008",
          "page_number": 5,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p5:b0009",
          "page_number": 5,
          "type": "text",
          "text": "responses. Referencing prior conversations makes dialogue more natural and engaging.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p5:b0010",
          "page_number": 5,
          "type": "text",
          "text": "We therefore assess our proposed system, MemGPT, on these two criteria: (1) Does MemGPT leverage its memory to improve conversation consistency? Can it remember relevant facts, preferences, and events from past interactions to maintain coherence? (2) Does MemGPT produce more engaging dialogue by taking advantage of memory? Does it spontaneously incorporate long-range user information to personalize messages? By evaluating on consistency and engagement, we can determine how well MemGPT handles the challenges of long-term conversational interaction compared to fixed-context baselines. Its ability to satisfy these criteria will demonstrate whether unbounded context provides meaningful benefits for conversational agents.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p5:b0011",
          "page_number": 5,
          "type": "text",
          "text": "### Dataset",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p5:b0012",
          "page_number": 5,
          "type": "text",
          "text": "We evaluate MemGPT and our fixed-context baselines on the Multi-Session Chat (MSC) dataset introduced by Xu et al. (2021), which contains multi-session chat logs generated by human labelers, each of whom was asked to play a consistent persona for the duration of all sessions. Each multi-session chat in MSC has five total sessions, and each session consists of a roughly a dozen messages. As part of our consistency experiments, we created a new session (session 6) that contains a single question-answer response pair between the same two personas.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p5:b0013",
          "page_number": 5,
          "type": "text",
          "text": "### 3.1.1. Deep Memory Retrieval Task (Consistency)",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p5:b0014",
          "page_number": 5,
          "type": "text",
          "text": "We introduce a new ‚Äòdeep memory retrieval‚Äô (DMR) task based on the MSC dataset designed to test the consistency of a conversational agent. In DMR, the conversational agent is asked a question by the user that explicitly refers back to a prior conversation and has a very narrow expected answer range. We generated the DMR question-answer (QA) pairs using a separate LLM that was instructed to write a question from one user to another that could only be answered correctly using knowledge gained from the past sessions (see Appendix for further details).",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p5:b0015",
          "page_number": 5,
          "type": "text",
          "text": "We evaluate the quality of the generated response against the ‚Äògold response‚Äô using ROUGE-L scores (Lin, 2004) and an ‚ÄòLLM judge‚Äô, which is instructed to evaluate whether or not the generated response is consistent with the gold response (GPT-4 has been shown to have high agreement with human evaluators (Zheng et al., 2023)). In practice, we notice that the generated responses (from both MemGPT and the baselines) were generally more verbose than the gold responses. We use the ROUGE-L recall (R) metric to account for the verbosity of the generated agent replies compared to the relatively short gold answer labels.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p5:b0016",
          "page_number": 5,
          "type": "text",
          "text": "**MemGPT utilizes memory to maintain coherence:** Table 2 shows the performance of MemGPT vs the fixed-memory baselines. We compare MemGPT using different underlying LLMs, and compare against using the base LLM without MemGPT as a baseline. The baselines are able to see a lossy summarization of the past five conversations to mimic an extended recursive summarization procedure, while MemGPT instead has access to the full conversation history but must access it via paginated search queries to recall memory (in order to bring them into main context). In this task, we see that MemGPT clearly improves the performance of the underlying base LLM: there is a clear drop in both accuracy and ROUGE scores when going from MemGPT to the corresponding LLM baselines.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p5:b0017",
          "page_number": 5,
          "type": "text",
          "text": "### 3.1.2. Conversation Opener Task (Engagement)",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p5:b0018",
          "page_number": 5,
          "type": "text",
          "text": "In the ‚Äòconversation opener‚Äô task we evaluate an agent‚Äôs ability to craft engaging messages to the user that draw from knowledge accumulated in prior conversations. To evaluate the ‚Äòengagingness‚Äô of a conversation opener using the MSC dataset, we compare the generated opener to the gold personas: an engaging conversation opener should draw from one (or several) of the data points contained in the persona, which in MSC effectively summarize the knowledge accumulated throughout all prior sessions. We also compare to the human-generated gold opener, i.e., the",
          "source": "ocr_md_rule"
        }
      ]
    },
    {
      "page_number": 6,
      "dpi": 144,
      "width_px": 1224,
      "height_px": 1584,
      "image_path": "/home/work/workspace/Think-with-Doc-Agent/demo/imgs/2310.08560v2/page_0006.png",
      "image_sha256": "sha256:05c8e5da1f1222bb000ba32a467943e357927c4c22b3cfaddb20b2a5e7bd6aaf",
      "renderer": "pymupdf",
      "colorspace": "rgb",
      "text_raw": "# MemGPT: Towards LLMs as Operating Systems\n\n## Figure 5. Document QA task performance\n\nMemGPT's performance is unaffected by increased context length. Methods such as truncation can extend the effective context lengths of fixed-length models such as GPT-4, but such compression methods will lead to performance degradation as the necessary compression grows. Running MemGPT with GPT-4 and GPT-4 Turbo have equivalent results on this task.\n\n![Figure 5: Document QA task performance](https://i.imgur.com/placeholder1.png)\n\n## Figure 6. An example of MemGPT (left) solving the document QA task\n\nA database of Wikipedia documents is uploaded to archival storage. MemGPT queries archival storage via function calling, which pulls paginated search results into main context.\n\n![Figure 6: Example of MemGPT solving document QA task](https://i.imgur.com/placeholder2.png)\n\n---\n\n## 3.2. MemGPT for document analysis\n\nDocument analysis also faces challenges due to the limited context windows of today‚Äôs transformer models. As shown in Table 1, both open and closed source models suffer from constrained context length (up to 128k tokens for OpenAI‚Äôs models). However, many documents easily surpass these lengths; for example, legal or financial documents such as Annual Reports (SEC Form 10-K) can easily pass the million token mark. Moreover, many real document analysis tasks require drawing connections across multiple such lengthy documents. Anticipating these scenarios, it becomes difficult to envision blindly scaling up context as a solution to the fixed-context problem. Recent research (Liu et al., 2023a) also raises doubts about the utility of simply scaling contexts, since they find uneven attention distributions in large context models (the model is more capable of recalling information at the beginning or end of its context window, vs tokens in the middle). To enable reasoning across documents, more flexible memory architectures like MemGPT are needed.\n\n### 3.2.1. MULTI-DOCUMENT QUESTION-ANSWERING\n\nTo evaluate MemGPT‚Äôs ability to analyze documents, we benchmark MemGPT against fixed-context baselines on the retriever-reader document QA task from Liu et al. (2023a). In this task, a question is selected from the NaturalQuestions-Open dataset, and a retriever selects relevant Wikipedia documents for the question. A reader model (the LLM) is then fed these documents as input, and is asked to use the provided documents to answer the question. Similar to Liu et al. (2023a), we evaluate reader accuracy as the number of retrieved documents $K$ increases.\n\nIn our evaluation setup, both the fixed-context baselines and MemGPT use the same retriever, which selects the top $K$ documents according to similarity search (cosine distance) on OpenAI‚Äôs `text-embedding-ada-002` embeddings. We use MemGPT‚Äôs default storage settings which uses PostgreSQL for archival memory storage with vector search enabled via the `pgvector` extension. We precompute embeddings and load them into the database, which uses an HNSW index to enable approximate, sub-second query times. In MemGPT, the entire embedding document set is loaded into archival storage, and the retriever naturally emerges via the archival storage search functionality (which performs vector search based on cosine similarity). In the fixed-context baselines, the top-$K$ documents are fetched using the retriever independently from the LLM inference, similar to the original retriever-reader setup in Liu et al. (2023a).\n\nWe use a dump of Wikipedia from late 2018, following past work on NaturalQuestions-Open (Izacard & Grave, 2020; ...).\n\n---\n\n## MemGPT utilizes memory to increase engagement\n\nAs seen in Table 3, MemGPT is able to craft engaging openers that perform similarly to and occasionally exceed the hand-written human openers. We observe that MemGPT tends to craft openers that are both more verbose and cover more aspects of the persona information than the human baseline. Additionally, we can see the storing information in working context is key to generating engaging openers.\n\n---\n\n## First Response in the Following Session\n\nWe report the CSIM scores of MemGPT‚Äôs openers in Table 3. We test several variations of MemGPT using different base LLMs.\n\n---",
      "text_source": "ocr",
      "spans": [],
      "diagnostics": {
        "ocr": {
          "model": "Qwen3-VL-32B-Instruct",
          "prompt": "qwenvl markdown",
          "min_pixels": 524288,
          "max_pixels": 4718592,
          "smart_resize": {
            "in_w": 1224,
            "in_h": 1584,
            "out_w": 1216,
            "out_h": 1600,
            "factor": 32
          },
          "elapsed_ms": 16070,
          "image_sha256": "sha256:05c8e5da1f1222bb000ba32a467943e357927c4c22b3cfaddb20b2a5e7bd6aaf"
        }
      },
      "page_summary": "- **Section 3.2** discusses challenges in document analysis due to limited context windows of transformer models (up to 128k tokens), noting that real-world documents (e.g., SEC Form 10-K) often exceed this, and that reasoning across multiple documents is difficult with fixed-context approaches.  \n- **Section 3.2.1** describes a multi-document QA evaluation where MemGPT is benchmarked against fixed-context baselines using the retriever-reader task from Liu et al. (2023a), with accuracy measured as the number of retrieved documents $K$ increases.  \n- The evaluation setup uses OpenAI‚Äôs `text-embedding-ada-002` for similarity search; MemGPT leverages PostgreSQL with `pgvector` and an HNSW index for fast archival storage retrieval, while fixed-context baselines fetch top-$K$ documents separately from LLM inference.  \n- **Figure 5** shows that MemGPT‚Äôs accuracy remains stable as context length increases, unlike truncation-based methods which degrade; it also performs equivalently with GPT-4 and GPT-4 Turbo.  \n- **Figure 6** illustrates MemGPT‚Äôs workflow: it queries archival storage (Wikipedia) via function calls to retrieve paginated results, which are then used to answer a question (e.g., ‚ÄúWho won the first Nobel Prize in physics?‚Äù).  \n- A separate section notes that MemGPT generates engaging openers (per Table 3) that are more verbose and comprehensive than human-written ones, highlighting the importance of storing information in working context.",
      "blocks": [
        {
          "block_id": "p6:b0001",
          "page_number": 6,
          "type": "text",
          "text": "# MemGPT: Towards LLMs as Operating Systems",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p6:b0002",
          "page_number": 6,
          "type": "text",
          "text": "## Figure 5. Document QA task performance",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p6:b0003",
          "page_number": 6,
          "type": "text",
          "text": "MemGPT's performance is unaffected by increased context length. Methods such as truncation can extend the effective context lengths of fixed-length models such as GPT-4, but such compression methods will lead to performance degradation as the necessary compression grows. Running MemGPT with GPT-4 and GPT-4 Turbo have equivalent results on this task.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p6:b0004",
          "page_number": 6,
          "type": "text",
          "text": "![Figure 5: Document QA task performance](https://i.imgur.com/placeholder1.png)",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p6:b0005",
          "page_number": 6,
          "type": "text",
          "text": "## Figure 6. An example of MemGPT (left) solving the document QA task",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p6:b0006",
          "page_number": 6,
          "type": "text",
          "text": "A database of Wikipedia documents is uploaded to archival storage. MemGPT queries archival storage via function calling, which pulls paginated search results into main context.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p6:b0007",
          "page_number": 6,
          "type": "text",
          "text": "![Figure 6: Example of MemGPT solving document QA task](https://i.imgur.com/placeholder2.png)",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p6:b0008",
          "page_number": 6,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p6:b0009",
          "page_number": 6,
          "type": "text",
          "text": "## 3.2. MemGPT for document analysis",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p6:b0010",
          "page_number": 6,
          "type": "text",
          "text": "Document analysis also faces challenges due to the limited context windows of today‚Äôs transformer models. As shown in Table 1, both open and closed source models suffer from constrained context length (up to 128k tokens for OpenAI‚Äôs models). However, many documents easily surpass these lengths; for example, legal or financial documents such as Annual Reports (SEC Form 10-K) can easily pass the million token mark. Moreover, many real document analysis tasks require drawing connections across multiple such lengthy documents. Anticipating these scenarios, it becomes difficult to envision blindly scaling up context as a solution to the fixed-context problem. Recent research (Liu et al., 2023a) also raises doubts about the utility of simply scaling contexts, since they find uneven attention distributions in large context models (the model is more capable of recalling information at the beginning or end of its context window, vs tokens in the middle). To enable reasoning across documents, more flexible memory architectures like MemGPT are needed.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p6:b0011",
          "page_number": 6,
          "type": "text",
          "text": "### 3.2.1. MULTI-DOCUMENT QUESTION-ANSWERING",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p6:b0012",
          "page_number": 6,
          "type": "text",
          "text": "To evaluate MemGPT‚Äôs ability to analyze documents, we benchmark MemGPT against fixed-context baselines on the retriever-reader document QA task from Liu et al. (2023a). In this task, a question is selected from the NaturalQuestions-Open dataset, and a retriever selects relevant Wikipedia documents for the question. A reader model (the LLM) is then fed these documents as input, and is asked to use the provided documents to answer the question. Similar to Liu et al. (2023a), we evaluate reader accuracy as the number of retrieved documents $K$ increases.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p6:b0013",
          "page_number": 6,
          "type": "text",
          "text": "In our evaluation setup, both the fixed-context baselines and MemGPT use the same retriever, which selects the top $K$ documents according to similarity search (cosine distance) on OpenAI‚Äôs `text-embedding-ada-002` embeddings. We use MemGPT‚Äôs default storage settings which uses PostgreSQL for archival memory storage with vector search enabled via the `pgvector` extension. We precompute embeddings and load them into the database, which uses an HNSW index to enable approximate, sub-second query times. In MemGPT, the entire embedding document set is loaded into archival storage, and the retriever naturally emerges via the archival storage search functionality (which performs vector search based on cosine similarity). In the fixed-context baselines, the top-$K$ documents are fetched using the retriever independently from the LLM inference, similar to the original retriever-reader setup in Liu et al. (2023a).",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p6:b0014",
          "page_number": 6,
          "type": "text",
          "text": "We use a dump of Wikipedia from late 2018, following past work on NaturalQuestions-Open (Izacard & Grave, 2020; ...).",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p6:b0015",
          "page_number": 6,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p6:b0016",
          "page_number": 6,
          "type": "text",
          "text": "## MemGPT utilizes memory to increase engagement",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p6:b0017",
          "page_number": 6,
          "type": "text",
          "text": "As seen in Table 3, MemGPT is able to craft engaging openers that perform similarly to and occasionally exceed the hand-written human openers. We observe that MemGPT tends to craft openers that are both more verbose and cover more aspects of the persona information than the human baseline. Additionally, we can see the storing information in working context is key to generating engaging openers.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p6:b0018",
          "page_number": 6,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p6:b0019",
          "page_number": 6,
          "type": "text",
          "text": "## First Response in the Following Session",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p6:b0020",
          "page_number": 6,
          "type": "text",
          "text": "We report the CSIM scores of MemGPT‚Äôs openers in Table 3. We test several variations of MemGPT using different base LLMs.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p6:b0021",
          "page_number": 6,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        }
      ]
    },
    {
      "page_number": 7,
      "dpi": 144,
      "width_px": 1224,
      "height_px": 1584,
      "image_path": "/home/work/workspace/Think-with-Doc-Agent/demo/imgs/2310.08560v2/page_0007.png",
      "image_sha256": "sha256:c4bb423734a2cbf0579aa00ec772e7d03b42b3648f55860c76df581a9b71e686",
      "renderer": "pymupdf",
      "colorspace": "rgb",
      "text_raw": "# MemGPT: Towards LLMs as Operating Systems\n\n## Figure 7. Nested KV retrieval task performance.\n\nMemGPT is the only approach that is able to consistently complete the nested KV task beyond 2 nesting levels. While GPT-4 Turbo performs better as a baseline, MemGPT with GPT-4 Turbo performs worse than MemGPT with GPT-4.\n\n---\n\n## Figure 8. An example of MemGPT (left) solving the nested KV task (UUIDs shortened for readability).\n\nIn this particular example, the key-value pair has two nesting levels: `831..ea5 ‚Üí 5b8..4c3 ‚Üí f37..617`. The MemGPT agent returns the final answer when a query for the final value (`f37..617`) only returns one result, indicating that it is not also a key.\n\n---\n\n## Text Content\n\nIzacard et al., 2021), and sampled a subset of 50 questions for evaluation. Both the sampled questions and embedded Wikipedia passages are publicly released. We evaluate the performance of both MemGPT and baselines with an LLM-judge, to ensure that the answer is properly derived from the retrieved documents and to avoid non-exact string matches being considered incorrect.\n\nWe show the results for the document QA task in Figure 5. The fixed-context baselines performance is capped roughly at the performance of the retriever, as they use the information that is presented in their context window (e.g., if the embedding search retriever fails to surface the gold article using the provided question, the fixed-context baselines are guaranteed to never see the gold article). By contrast, MemGPT is effectively able to make multiple calls to the retriever by querying archival storage, allowing it to scale to larger effective context lengths. MemGPT actively retrieves documents from its archival storage (and can iteratively page through results), so the total number of documents available to MemGPT is no longer limited by the number of documents that fit within the LLM processor‚Äôs context window.\n\nThe document QA task is challenging for all methods due to the limitations of embedding-based similarity search. We observe that the golden document for chosen question (as annotated by NaturalQuestions-Open) often appears outside of the first dozen retrieved results, if not even further. The retriever performance translates directly to the fixed-context baseline results: GPT-4‚Äôs accuracy is relatively low with few retrieved documents, and continues to improve as additional documents are added to the context window, as it correctly limits itself to answering questions based on information in retrieved documents. While MemGPT is theoretically not limited by sub-optimal retriever performance (even if the embedding-based ranking is noisy, as long as the full retriever ranking contains the gold document it can still be found with enough retriever calls via pagination), we observe that MemGPT will often stop paging through retriever results before exhausting the retriever database.\n\nTo evaluate the fixed-context baselines against MemGPT past their default context lengths, we truncate the document segments returned by the retriever to fix the same number of documents into the available context. As expected, document truncation reduces accuracy as documents shrink as the chance of the relevant snippet (in the gold document) being omitted grows, as shown in Figure 5. MemGPT has significantly degraded performance using GPT-3.5, due to its limited function calling capabilities, and performs best using GPT-4.\n\n### 3.2.2. NESTED KEY-VALUE RETRIEVAL (KV)\n\nWe introduce a new task based on the synthetic Key-Value retrieval proposed in prior work (Liu et al., 2023a). The goal of this task is to demonstrate how MemGPT can collate information from multiple data sources. In the original KV task, the authors generated a synthetic dataset of key-value pairs, where each key and value is a 128-bit UUID (universally unique identifier). The agent is then given a key, and asked to return the associated value for the key. We create a version of the KV task, *nested KV retrieval*,",
      "text_source": "ocr",
      "spans": [],
      "diagnostics": {
        "ocr": {
          "model": "Qwen3-VL-32B-Instruct",
          "prompt": "qwenvl markdown",
          "min_pixels": 524288,
          "max_pixels": 4718592,
          "smart_resize": {
            "in_w": 1224,
            "in_h": 1584,
            "out_w": 1216,
            "out_h": 1600,
            "factor": 32
          },
          "elapsed_ms": 15113,
          "image_sha256": "sha256:c4bb423734a2cbf0579aa00ec772e7d03b42b3648f55860c76df581a9b71e686"
        }
      },
      "page_summary": "- **Section 3.2.2 (Nested Key-Value Retrieval)** introduces a new synthetic task where an agent must retrieve values from a nested key-value structure (e.g., `831..ea5 ‚Üí 5b8..4c3 ‚Üí f37..617`), demonstrating MemGPT‚Äôs ability to collate information across multiple data sources.\n\n- **Figure 7** shows that MemGPT is the only approach capable of consistently completing the nested KV task beyond 2 nesting levels, outperforming baselines like GPT-3.5, GPT-4 Turbo, and GPT-4. Notably, MemGPT with GPT-4 performs better than MemGPT with GPT-4 Turbo.\n\n- **Figure 8** provides an example of MemGPT solving a nested KV task, illustrating how it iteratively queries archival storage to resolve the chain of keys until it finds a value that is not itself a key (e.g., `f37..617`).\n\n- The document QA task (results in Figure 5, not shown) highlights that fixed-context baselines are limited by the retriever‚Äôs ability to surface relevant documents within the context window, whereas MemGPT can scale beyond this limit by making multiple retriever calls.\n\n- MemGPT‚Äôs performance degrades with GPT-3.5 due to limited function calling capabilities but performs best with GPT-4. It often stops paging through retriever results before exhausting the database, even when theoretically capable of doing so.",
      "blocks": [
        {
          "block_id": "p7:b0001",
          "page_number": 7,
          "type": "text",
          "text": "# MemGPT: Towards LLMs as Operating Systems",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p7:b0002",
          "page_number": 7,
          "type": "text",
          "text": "## Figure 7. Nested KV retrieval task performance.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p7:b0003",
          "page_number": 7,
          "type": "text",
          "text": "MemGPT is the only approach that is able to consistently complete the nested KV task beyond 2 nesting levels. While GPT-4 Turbo performs better as a baseline, MemGPT with GPT-4 Turbo performs worse than MemGPT with GPT-4.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p7:b0004",
          "page_number": 7,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p7:b0005",
          "page_number": 7,
          "type": "text",
          "text": "## Figure 8. An example of MemGPT (left) solving the nested KV task (UUIDs shortened for readability).",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p7:b0006",
          "page_number": 7,
          "type": "text",
          "text": "In this particular example, the key-value pair has two nesting levels: `831..ea5 ‚Üí 5b8..4c3 ‚Üí f37..617`. The MemGPT agent returns the final answer when a query for the final value (`f37..617`) only returns one result, indicating that it is not also a key.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p7:b0007",
          "page_number": 7,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p7:b0008",
          "page_number": 7,
          "type": "text",
          "text": "## Text Content",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p7:b0009",
          "page_number": 7,
          "type": "text",
          "text": "Izacard et al., 2021), and sampled a subset of 50 questions for evaluation. Both the sampled questions and embedded Wikipedia passages are publicly released. We evaluate the performance of both MemGPT and baselines with an LLM-judge, to ensure that the answer is properly derived from the retrieved documents and to avoid non-exact string matches being considered incorrect.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p7:b0010",
          "page_number": 7,
          "type": "text",
          "text": "We show the results for the document QA task in Figure 5. The fixed-context baselines performance is capped roughly at the performance of the retriever, as they use the information that is presented in their context window (e.g., if the embedding search retriever fails to surface the gold article using the provided question, the fixed-context baselines are guaranteed to never see the gold article). By contrast, MemGPT is effectively able to make multiple calls to the retriever by querying archival storage, allowing it to scale to larger effective context lengths. MemGPT actively retrieves documents from its archival storage (and can iteratively page through results), so the total number of documents available to MemGPT is no longer limited by the number of documents that fit within the LLM processor‚Äôs context window.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p7:b0011",
          "page_number": 7,
          "type": "text",
          "text": "The document QA task is challenging for all methods due to the limitations of embedding-based similarity search. We observe that the golden document for chosen question (as annotated by NaturalQuestions-Open) often appears outside of the first dozen retrieved results, if not even further. The retriever performance translates directly to the fixed-context baseline results: GPT-4‚Äôs accuracy is relatively low with few retrieved documents, and continues to improve as additional documents are added to the context window, as it correctly limits itself to answering questions based on information in retrieved documents. While MemGPT is theoretically not limited by sub-optimal retriever performance (even if the embedding-based ranking is noisy, as long as the full retriever ranking contains the gold document it can still be found with enough retriever calls via pagination), we observe that MemGPT will often stop paging through retriever results before exhausting the retriever database.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p7:b0012",
          "page_number": 7,
          "type": "text",
          "text": "To evaluate the fixed-context baselines against MemGPT past their default context lengths, we truncate the document segments returned by the retriever to fix the same number of documents into the available context. As expected, document truncation reduces accuracy as documents shrink as the chance of the relevant snippet (in the gold document) being omitted grows, as shown in Figure 5. MemGPT has significantly degraded performance using GPT-3.5, due to its limited function calling capabilities, and performs best using GPT-4.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p7:b0013",
          "page_number": 7,
          "type": "text",
          "text": "### 3.2.2. NESTED KEY-VALUE RETRIEVAL (KV)",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p7:b0014",
          "page_number": 7,
          "type": "text",
          "text": "We introduce a new task based on the synthetic Key-Value retrieval proposed in prior work (Liu et al., 2023a). The goal of this task is to demonstrate how MemGPT can collate information from multiple data sources. In the original KV task, the authors generated a synthetic dataset of key-value pairs, where each key and value is a 128-bit UUID (universally unique identifier). The agent is then given a key, and asked to return the associated value for the key. We create a version of the KV task, *nested KV retrieval*,",
          "source": "ocr_md_rule"
        }
      ]
    },
    {
      "page_number": 8,
      "dpi": 144,
      "width_px": 1224,
      "height_px": 1584,
      "image_path": "/home/work/workspace/Think-with-Doc-Agent/demo/imgs/2310.08560v2/page_0008.png",
      "image_sha256": "sha256:592b071f90b5b3c4a4a5e150a50394667ed5bddad969346661c6709aed47fe42",
      "renderer": "pymupdf",
      "colorspace": "rgb",
      "text_raw": "# MemGPT: Towards LLMs as Operating Systems\n\nwhere values themselves may be keys, thus requiring the agent to perform a multi-hop lookup. In our setup, we fix the total number of UUIDs pairs to 140, corresponding to roughly 8k tokens (the context length of our GPT-4 baseline). We vary the total number of nesting levels from 0 (the initial key-value pair‚Äôs value is not a key) to 4 (ie 4 total KV lookups are required to find the final value), and sample 30 different ordering configurations including both the initial key position and nesting key positions.\n\nWhile GPT-3.5 and GPT-4 have good performance on the original KV tasks, both struggle in the nested KV task. GPT-3.5 is unable to complete the nested variant of the task and has an immediate dropoff in performance, hitting 0 percent accuracy at 1 nesting level (we observe that its primary failure mode is to simply returns the original value). GPT-4 and GPT-4 Turbo are better than GPT-3.5, but also suffer from a similar dropoff, and hit 0 percent accuracy by 3 nesting levels. MemGPT on the other hand is unaffected with the number of nesting levels and is able to perform the nested lookup by accessing the key-value pairs stored in main context repeatedly via function queries. MemGPT with GPT-4 Turbo and GPT-3.5 also have better performance than the corresponding baseline models, but still begin to drop off in performance at 2 nesting levels as a result of failing to perform enough lookups. MemGPT performance on the nested KV task demonstrates its ability to combine multiple queries to perform multi-hop lookups.\n\n## 4. Related Work\n\n### Long-context LLMs.\nSeveral lines of work have improved the context length of LLMs. For instance, more efficient transformer architectures via sparsifying the attention (Child et al., 2019; Beltagy et al., 2020), low-rank approximations (Wang et al., 2020), and neural memory (Lee et al., 2019). Another line of work aims to extend context windows beyond the length they were original trained for, their training size, such as Press et al. (2021); Chen et al. (2023). MemGPT builds upon these improvements in context length as they improve the size of the main memory in MemGPT. Our main contribution is a hierarchical tiered memory that uses a long-context LLM as the implementation of main memory.\n\n### Retrieval-Augmented Models.\nThe design of the external memory of MemGPT builds upon much prior work augmenting LLMs with relevant inputs from external retrievers (Ram et al., 2023; Borgeaud et al., 2022; Karpathy et al., 2020; Lewis et al., 2020; Guu et al., 2020; Lin et al., 2023). In particular, Jiang et al. (2023) propose FLARE, a method that allows the LLM to actively decide when and what to retrieve during the course of generation. Trivedi et al. (2022) interleave retrieval with Chain-of-Thoughts reasoning to improve multi-step question answering.\n\n### LLMs as agents.\nRecent work has explored augmenting LLMs with additional capabilities to act as agents in interactive environments. Park et al. (2023) propose adding memory to LLMs and using the LLM as a planner, and observe emergent social behaviors in a multi-agent sandbox environment (inspired by *The Sims* video game) where agents can perform basic activities such as doing chores/hobbies, going to work, and conversing with other agents. Nakano et al. (2021) train models to search the web before answering questions, and use similar pagination concepts to MemGPT to control the underlying context size in their web-browsing environment. Yao et al. (2022) showed that interleaving chain-of-thought reasoning (Wei et al., 2022) can further improve the planning ability of interactive LLM-based agents; similarly in MemGPT, LLM is able to ‚Äòplan out loud‚Äô when executing functions. Liu et al. (2023b) introduced a suite of LLM-as-an-agent benchmarks to evaluate LLMs in interactive environments, including video games, thinking puzzles, and web shopping. In contrast, our work focuses on tackling the problem of equipping agents with long-term memory of user inputs.\n\n## 5. Conclusion\n\nIn this paper, we introduced MemGPT, a novel LLM system inspired by operating systems to manage the limited context windows of large language models. By designing a memory hierarchy and control flow analogous to traditional OSes, MemGPT provides the illusion of larger context resources for LLMs. This OS-inspired approach was evaluated in two domains where existing LLM performance is constrained by finite context lengths: document analysis and conversational agents. For document analysis, MemGPT could process lengthy texts well beyond the context limits of current LLMs by effectively paging relevant context in and out of memory. For conversational agents, MemGPT enabled maintaining long-term memory, consistency, and evolvability over extended dialogues. Overall, MemGPT demonstrates that operating system techniques like hierarchical memory management and interrupts can unlock the potential of LLMs even when constrained by fixed context lengths. This work opens numerous avenues for future exploration, including applying MemGPT to other domains with massive or unbounded contexts, integrating different memory tier technologies like databases or caches, and further improving control flow and memory management policies. By bridging concepts from OS architecture into AI systems, MemGPT represents a promising new direction for maximizing the capabilities of LLMs within their fundamental limits.",
      "text_source": "ocr",
      "spans": [],
      "diagnostics": {
        "ocr": {
          "model": "Qwen3-VL-32B-Instruct",
          "prompt": "qwenvl markdown",
          "min_pixels": 524288,
          "max_pixels": 4718592,
          "smart_resize": {
            "in_w": 1224,
            "in_h": 1584,
            "out_w": 1216,
            "out_h": 1600,
            "factor": 32
          },
          "elapsed_ms": 21744,
          "image_sha256": "sha256:592b071f90b5b3c4a4a5e150a50394667ed5bddad969346661c6709aed47fe42"
        }
      },
      "page_summary": "- **Nested KV Task Performance**: MemGPT outperforms GPT-3.5 and GPT-4 in nested key-value lookup tasks, maintaining accuracy across up to 4 nesting levels, while baseline models fail after 1‚Äì3 levels due to inability to perform multi-hop lookups.\n- **MemGPT‚Äôs Mechanism**: It achieves this by repeatedly querying key-value pairs stored in main memory via function calls, demonstrating its ability to combine multiple queries for complex lookups.\n- **Related Work (Section 4)**: The paper reviews prior work in long-context LLMs (e.g., sparse attention, neural memory), retrieval-augmented models (e.g., FLARE, interleaved retrieval with CoT), and LLMs as agents (e.g., memory-augmented planning, web browsing, multi-agent environments), positioning MemGPT as a novel OS-inspired memory system.\n- **Conclusion (Section 5)**: MemGPT is presented as an LLM system inspired by operating systems, using hierarchical memory and control flow to overcome context length limitations in document analysis and conversational agents.\n- **Key Contributions**: MemGPT enables processing of lengthy texts and long-term memory in dialogues by ‚Äúpaging‚Äù context in and out of memory, and the paper suggests future work in integrating databases, caches, and improved memory policies.\n- **Overall Impact**: The work argues that OS techniques like hierarchical memory management can unlock LLM potential within fixed context limits, representing a promising new direction for AI systems.",
      "blocks": [
        {
          "block_id": "p8:b0001",
          "page_number": 8,
          "type": "text",
          "text": "# MemGPT: Towards LLMs as Operating Systems",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p8:b0002",
          "page_number": 8,
          "type": "text",
          "text": "where values themselves may be keys, thus requiring the agent to perform a multi-hop lookup. In our setup, we fix the total number of UUIDs pairs to 140, corresponding to roughly 8k tokens (the context length of our GPT-4 baseline). We vary the total number of nesting levels from 0 (the initial key-value pair‚Äôs value is not a key) to 4 (ie 4 total KV lookups are required to find the final value), and sample 30 different ordering configurations including both the initial key position and nesting key positions.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p8:b0003",
          "page_number": 8,
          "type": "text",
          "text": "While GPT-3.5 and GPT-4 have good performance on the original KV tasks, both struggle in the nested KV task. GPT-3.5 is unable to complete the nested variant of the task and has an immediate dropoff in performance, hitting 0 percent accuracy at 1 nesting level (we observe that its primary failure mode is to simply returns the original value). GPT-4 and GPT-4 Turbo are better than GPT-3.5, but also suffer from a similar dropoff, and hit 0 percent accuracy by 3 nesting levels. MemGPT on the other hand is unaffected with the number of nesting levels and is able to perform the nested lookup by accessing the key-value pairs stored in main context repeatedly via function queries. MemGPT with GPT-4 Turbo and GPT-3.5 also have better performance than the corresponding baseline models, but still begin to drop off in performance at 2 nesting levels as a result of failing to perform enough lookups. MemGPT performance on the nested KV task demonstrates its ability to combine multiple queries to perform multi-hop lookups.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p8:b0004",
          "page_number": 8,
          "type": "text",
          "text": "## 4. Related Work",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p8:b0005",
          "page_number": 8,
          "type": "text",
          "text": "### Long-context LLMs.\nSeveral lines of work have improved the context length of LLMs. For instance, more efficient transformer architectures via sparsifying the attention (Child et al., 2019; Beltagy et al., 2020), low-rank approximations (Wang et al., 2020), and neural memory (Lee et al., 2019). Another line of work aims to extend context windows beyond the length they were original trained for, their training size, such as Press et al. (2021); Chen et al. (2023). MemGPT builds upon these improvements in context length as they improve the size of the main memory in MemGPT. Our main contribution is a hierarchical tiered memory that uses a long-context LLM as the implementation of main memory.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p8:b0006",
          "page_number": 8,
          "type": "text",
          "text": "### Retrieval-Augmented Models.\nThe design of the external memory of MemGPT builds upon much prior work augmenting LLMs with relevant inputs from external retrievers (Ram et al., 2023; Borgeaud et al., 2022; Karpathy et al., 2020; Lewis et al., 2020; Guu et al., 2020; Lin et al., 2023). In particular, Jiang et al. (2023) propose FLARE, a method that allows the LLM to actively decide when and what to retrieve during the course of generation. Trivedi et al. (2022) interleave retrieval with Chain-of-Thoughts reasoning to improve multi-step question answering.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p8:b0007",
          "page_number": 8,
          "type": "text",
          "text": "### LLMs as agents.\nRecent work has explored augmenting LLMs with additional capabilities to act as agents in interactive environments. Park et al. (2023) propose adding memory to LLMs and using the LLM as a planner, and observe emergent social behaviors in a multi-agent sandbox environment (inspired by *The Sims* video game) where agents can perform basic activities such as doing chores/hobbies, going to work, and conversing with other agents. Nakano et al. (2021) train models to search the web before answering questions, and use similar pagination concepts to MemGPT to control the underlying context size in their web-browsing environment. Yao et al. (2022) showed that interleaving chain-of-thought reasoning (Wei et al., 2022) can further improve the planning ability of interactive LLM-based agents; similarly in MemGPT, LLM is able to ‚Äòplan out loud‚Äô when executing functions. Liu et al. (2023b) introduced a suite of LLM-as-an-agent benchmarks to evaluate LLMs in interactive environments, including video games, thinking puzzles, and web shopping. In contrast, our work focuses on tackling the problem of equipping agents with long-term memory of user inputs.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p8:b0008",
          "page_number": 8,
          "type": "text",
          "text": "## 5. Conclusion",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p8:b0009",
          "page_number": 8,
          "type": "text",
          "text": "In this paper, we introduced MemGPT, a novel LLM system inspired by operating systems to manage the limited context windows of large language models. By designing a memory hierarchy and control flow analogous to traditional OSes, MemGPT provides the illusion of larger context resources for LLMs. This OS-inspired approach was evaluated in two domains where existing LLM performance is constrained by finite context lengths: document analysis and conversational agents. For document analysis, MemGPT could process lengthy texts well beyond the context limits of current LLMs by effectively paging relevant context in and out of memory. For conversational agents, MemGPT enabled maintaining long-term memory, consistency, and evolvability over extended dialogues. Overall, MemGPT demonstrates that operating system techniques like hierarchical memory management and interrupts can unlock the potential of LLMs even when constrained by fixed context lengths.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p8:b0010",
          "page_number": 8,
          "type": "text",
          "text": "This work opens numerous avenues for future exploration, including applying MemGPT to other domains with massive or unbounded contexts, integrating different memory tier technologies like databases or caches, and further improving control flow and memory management policies. By bridging concepts from OS architecture into AI systems, MemGPT represents a promising new direction for maximizing the capabilities of LLMs within their fundamental limits.",
          "source": "ocr_md_rule"
        }
      ]
    },
    {
      "page_number": 9,
      "dpi": 144,
      "width_px": 1224,
      "height_px": 1584,
      "image_path": "/home/work/workspace/Think-with-Doc-Agent/demo/imgs/2310.08560v2/page_0009.png",
      "image_sha256": "sha256:2c1e7dd319041dbb927db30dd47b97b78c52db69fe22a112920f47ed4173520d",
      "renderer": "pymupdf",
      "colorspace": "rgb",
      "text_raw": "# References\n\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. *arXiv preprint arXiv:2004.05150*, 2020.\n\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In *International conference on machine learning*, pp. 2206‚Äì2240. PMLR, 2022.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. *Advances in neural information processing systems*, 33: 1877‚Äì1901, 2020.\n\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuangdong Tian. Extending context window of large language models via positional interpolation. *arXiv preprint arXiv:2306.15595*, 2023.\n\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. *arXiv preprint arXiv:1904.10509*, 2019.\n\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. *arXiv preprint arXiv:1901.02860*, 2019.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. *arXiv preprint arXiv:1810.04805*, 2018.\n\nZican Dong, Tianyi Tang, Lunyi Li, and Wayne Xin Zhao. A survey on long text modeling with transformers. *arXiv preprint arXiv:2302.14502*, 2023.\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In *International conference on machine learning*, pp. 3929‚Äì3938. PMLR, 2020.\n\nGautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. *arXiv preprint arXiv:2007.01282*, 2020.\n\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. *arXiv preprint arXiv:2112.09118*, 2021.\n\nZhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. *arXiv preprint arXiv:2305.06983*, 2023.\n\nVladimir Karpukhin, Barlas Oƒüuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. *arXiv preprint arXiv:2004.04906*, 2020.\n\nNikita Kitaev, ≈Åukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. *arXiv preprint arXiv:2001.04451*, 2020.\n\nJuho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In *International conference on machine learning*, pp. 3744‚Äì3753. PMLR, 2019.\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. *Advances in Neural Information Processing Systems*, 33:9459‚Äì9474, 2020.\n\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In *Text summarization branches out*, pp. 74‚Äì81, 2004.\n\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilv√°ssy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. Ra-dit: Retrieval-augmented dual instruction tuning, 2023.\n\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. *arXiv preprint arXiv:2307.03172*, 2023a.\n\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. AgentBench: Evaluating llms as agents. *arXiv preprint arXiv:2308.03688*, 2023b.\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vincent Kosaraju, William Saunders, et al. WebGPT: Browser-assisted question-answering with human feedback. *arXiv preprint arXiv:2112.09332*, 2021.\n\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human",
      "text_source": "ocr",
      "spans": [],
      "diagnostics": {
        "ocr": {
          "model": "Qwen3-VL-32B-Instruct",
          "prompt": "qwenvl markdown",
          "min_pixels": 524288,
          "max_pixels": 4718592,
          "smart_resize": {
            "in_w": 1224,
            "in_h": 1584,
            "out_w": 1216,
            "out_h": 1600,
            "factor": 32
          },
          "elapsed_ms": 26844,
          "image_sha256": "sha256:2c1e7dd319041dbb927db30dd47b97b78c52db69fe22a112920f47ed4173520d"
        }
      },
      "page_summary": "This page (Page 9) is entirely composed of a **References** section, listing 23 academic papers and preprints related to large language models, transformers, retrieval-augmented generation, and long-context modeling.\n\nThe references cover a range of key topics, including:\n- **Long-context modeling** (e.g., Longformer, Transformer-XL, Reformer, Set Transformer, positional interpolation).\n- **Retrieval-augmented generation (RAG)** and dense passage retrieval for open-domain question answering (e.g., works by Karpukhin et al., Lewis et al., Izacard & Grave, Jiang et al.).\n- **Large language model pre-training and instruction tuning** (e.g., BERT, GPT-3, WebGPT, Ra-dit, AgentBench).\n- **Evaluation and analysis** of LLMs (e.g., ROUGE, \"Lost in the middle\" study on context usage).\n\nThe cited works are primarily from top conferences (ICML, NeurIPS) and arXiv preprints, spanning from 2018 to 2023, indicating a focus on recent advancements in the field.\n\nThere are no figures, tables, or main claims presented on this page‚Äîits purpose is solely to provide citations for prior work discussed in the paper.",
      "blocks": [
        {
          "block_id": "p9:b0001",
          "page_number": 9,
          "type": "text",
          "text": "# References",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0002",
          "page_number": 9,
          "type": "text",
          "text": "Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. *arXiv preprint arXiv:2004.05150*, 2020.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0003",
          "page_number": 9,
          "type": "text",
          "text": "Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In *International conference on machine learning*, pp. 2206‚Äì2240. PMLR, 2022.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0004",
          "page_number": 9,
          "type": "text",
          "text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. *Advances in neural information processing systems*, 33: 1877‚Äì1901, 2020.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0005",
          "page_number": 9,
          "type": "text",
          "text": "Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuangdong Tian. Extending context window of large language models via positional interpolation. *arXiv preprint arXiv:2306.15595*, 2023.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0006",
          "page_number": 9,
          "type": "text",
          "text": "Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. *arXiv preprint arXiv:1904.10509*, 2019.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0007",
          "page_number": 9,
          "type": "text",
          "text": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. *arXiv preprint arXiv:1901.02860*, 2019.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0008",
          "page_number": 9,
          "type": "text",
          "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. *arXiv preprint arXiv:1810.04805*, 2018.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0009",
          "page_number": 9,
          "type": "text",
          "text": "Zican Dong, Tianyi Tang, Lunyi Li, and Wayne Xin Zhao. A survey on long text modeling with transformers. *arXiv preprint arXiv:2302.14502*, 2023.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0010",
          "page_number": 9,
          "type": "text",
          "text": "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In *International conference on machine learning*, pp. 3929‚Äì3938. PMLR, 2020.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0011",
          "page_number": 9,
          "type": "text",
          "text": "Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. *arXiv preprint arXiv:2007.01282*, 2020.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0012",
          "page_number": 9,
          "type": "text",
          "text": "Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. *arXiv preprint arXiv:2112.09118*, 2021.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0013",
          "page_number": 9,
          "type": "text",
          "text": "Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. *arXiv preprint arXiv:2305.06983*, 2023.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0014",
          "page_number": 9,
          "type": "text",
          "text": "Vladimir Karpukhin, Barlas Oƒüuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. *arXiv preprint arXiv:2004.04906*, 2020.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0015",
          "page_number": 9,
          "type": "text",
          "text": "Nikita Kitaev, ≈Åukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. *arXiv preprint arXiv:2001.04451*, 2020.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0016",
          "page_number": 9,
          "type": "text",
          "text": "Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In *International conference on machine learning*, pp. 3744‚Äì3753. PMLR, 2019.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0017",
          "page_number": 9,
          "type": "text",
          "text": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. *Advances in Neural Information Processing Systems*, 33:9459‚Äì9474, 2020.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0018",
          "page_number": 9,
          "type": "text",
          "text": "Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In *Text summarization branches out*, pp. 74‚Äì81, 2004.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0019",
          "page_number": 9,
          "type": "text",
          "text": "Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilv√°ssy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. Ra-dit: Retrieval-augmented dual instruction tuning, 2023.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0020",
          "page_number": 9,
          "type": "text",
          "text": "Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. *arXiv preprint arXiv:2307.03172*, 2023a.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0021",
          "page_number": 9,
          "type": "text",
          "text": "Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. AgentBench: Evaluating llms as agents. *arXiv preprint arXiv:2308.03688*, 2023b.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0022",
          "page_number": 9,
          "type": "text",
          "text": "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vincent Kosaraju, William Saunders, et al. WebGPT: Browser-assisted question-answering with human feedback. *arXiv preprint arXiv:2112.09332*, 2021.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p9:b0023",
          "page_number": 9,
          "type": "text",
          "text": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human",
          "source": "ocr_md_rule"
        }
      ]
    },
    {
      "page_number": 10,
      "dpi": 144,
      "width_px": 1224,
      "height_px": 1584,
      "image_path": "/home/work/workspace/Think-with-Doc-Agent/demo/imgs/2310.08560v2/page_0010.png",
      "image_sha256": "sha256:9cda226fe97c931d9845405d26d2eade4584b64207d9b856add3693f82d89f7b",
      "renderer": "pymupdf",
      "colorspace": "rgb",
      "text_raw": "# MemGPT: Towards LLMs as Operating Systems\n\n## References\n\n- **feedback.** *Advances in Neural Information Processing Systems*, 35:27730‚Äì27744, 2022.\n\n- **Joon Sung Park, Joseph C O‚ÄôBrien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein.** Generative agents: Interactive simulacra of human behavior. *arXiv preprint arXiv:2304.03442*, 2023.\n\n- **David A Patterson, Garth Gibson, and Randy H Katz.** A case for redundant arrays of inexpensive disks (raid). In *Proceedings of the 1988 ACM SIGMOD international conference on Management of data*, pp. 109‚Äì116, 1988.\n\n- **Ofir Press, Noah A Smith, and Mike Lewis.** Train short, test long: Attention with linear biases enables input length extrapolation. *arXiv preprint arXiv:2108.12409*, 2021.\n\n- **Ori Ram, Yoav Levine, Itay Dalmiedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.** In-context retrieval-augmented language models. *arXiv preprint arXiv:2302.00083*, 2023.\n\n- **Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.** Toolformer: Language models can teach themselves to use tools. *arXiv preprint arXiv:2302.04761*, 2023.\n\n- **Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.** Llama 2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*, 2023.\n\n- **H. Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.** Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. *ArXiv*, abs/2212.10509, 2022.  \n  URL: https://api.semanticscholar.org/CorpusID:254877499.\n\n- **Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin.** Attention is all you need. *Advances in neural information processing systems*, 30, 2017.\n\n- **Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma.** Linformer: Self-attention with linear complexity. *arXiv preprint arXiv:2006.04768*, 2020.\n\n- **Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.** Chain-of-thought prompting elicits reasoning in large language models. *Advances in Neural Information Processing Systems*, 35:24824‚Äì24837, 2022.\n\n- **Jing Xu, Arthur Szlam, and Jason Weston.** Beyond goldfish memory: Long-term open-domain conversation. *arXiv preprint arXiv:2107.07567*, 2021.\n\n- **Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.** React: Synergizing reasoning and acting in language models. *arXiv preprint arXiv:2210.03629*, 2022.\n\n- **Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.** Judging llm-as-a-judge with mt-bench and chatbot arena. *arXiv preprint arXiv:2306.05685*, 2023.\n\n---\nPage: 10",
      "text_source": "ocr",
      "spans": [],
      "diagnostics": {
        "ocr": {
          "model": "Qwen3-VL-32B-Instruct",
          "prompt": "qwenvl markdown",
          "min_pixels": 524288,
          "max_pixels": 4718592,
          "smart_resize": {
            "in_w": 1224,
            "in_h": 1584,
            "out_w": 1216,
            "out_h": 1600,
            "factor": 32
          },
          "elapsed_ms": 18644,
          "image_sha256": "sha256:9cda226fe97c931d9845405d26d2eade4584b64207d9b856add3693f82d89f7b"
        }
      },
      "page_summary": "This page (Page 10) is entirely composed of a reference list for the paper \"MemGPT: Towards LLMs as Operating Systems\". It contains 15 citations to academic works, primarily from 2021‚Äì2023, with one from 1988. The references cover a range of topics relevant to large language models, including:\n\n- **LLM capabilities and reasoning**: Works on chain-of-thought prompting (Wei et al., 2022), reasoning and acting (Yao et al., 2022), and tool use (Schick et al., 2023).\n- **Memory and context**: Papers on long-term conversation (Xu et al., 2021), in-context retrieval (Ram et al., 2023), and memory-augmented models.\n- **Model architectures and efficiency**: References to attention mechanisms (Vaswani et al., 2017), linear attention (Wang et al., 2020), and input length extrapolation (Press et al., 2021).\n- **Model evaluation and benchmarks**: A citation to a paper on judging LLMs using mt-bench and chatbot arena (Zheng et al., 2023).\n- **Other relevant systems**: Includes foundational work on RAID (Patterson et al., 1988) and generative agents (Park et al., 2023).\n\nThe page does not contain any figures, tables, or original content‚Äîonly a formatted list of references.",
      "blocks": [
        {
          "block_id": "p10:b0001",
          "page_number": 10,
          "type": "text",
          "text": "# MemGPT: Towards LLMs as Operating Systems",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p10:b0002",
          "page_number": 10,
          "type": "text",
          "text": "## References",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p10:b0003",
          "page_number": 10,
          "type": "text",
          "text": "- **feedback.** *Advances in Neural Information Processing Systems*, 35:27730‚Äì27744, 2022.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p10:b0004",
          "page_number": 10,
          "type": "text",
          "text": "- **Joon Sung Park, Joseph C O‚ÄôBrien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein.** Generative agents: Interactive simulacra of human behavior. *arXiv preprint arXiv:2304.03442*, 2023.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p10:b0005",
          "page_number": 10,
          "type": "text",
          "text": "- **David A Patterson, Garth Gibson, and Randy H Katz.** A case for redundant arrays of inexpensive disks (raid). In *Proceedings of the 1988 ACM SIGMOD international conference on Management of data*, pp. 109‚Äì116, 1988.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p10:b0006",
          "page_number": 10,
          "type": "text",
          "text": "- **Ofir Press, Noah A Smith, and Mike Lewis.** Train short, test long: Attention with linear biases enables input length extrapolation. *arXiv preprint arXiv:2108.12409*, 2021.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p10:b0007",
          "page_number": 10,
          "type": "text",
          "text": "- **Ori Ram, Yoav Levine, Itay Dalmiedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.** In-context retrieval-augmented language models. *arXiv preprint arXiv:2302.00083*, 2023.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p10:b0008",
          "page_number": 10,
          "type": "text",
          "text": "- **Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.** Toolformer: Language models can teach themselves to use tools. *arXiv preprint arXiv:2302.04761*, 2023.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p10:b0009",
          "page_number": 10,
          "type": "text",
          "text": "- **Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.** Llama 2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*, 2023.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p10:b0010",
          "page_number": 10,
          "type": "text",
          "text": "- **H. Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.** Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. *ArXiv*, abs/2212.10509, 2022.  \n  URL: https://api.semanticscholar.org/CorpusID:254877499.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p10:b0011",
          "page_number": 10,
          "type": "text",
          "text": "- **Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin.** Attention is all you need. *Advances in neural information processing systems*, 30, 2017.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p10:b0012",
          "page_number": 10,
          "type": "text",
          "text": "- **Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma.** Linformer: Self-attention with linear complexity. *arXiv preprint arXiv:2006.04768*, 2020.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p10:b0013",
          "page_number": 10,
          "type": "text",
          "text": "- **Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.** Chain-of-thought prompting elicits reasoning in large language models. *Advances in Neural Information Processing Systems*, 35:24824‚Äì24837, 2022.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p10:b0014",
          "page_number": 10,
          "type": "text",
          "text": "- **Jing Xu, Arthur Szlam, and Jason Weston.** Beyond goldfish memory: Long-term open-domain conversation. *arXiv preprint arXiv:2107.07567*, 2021.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p10:b0015",
          "page_number": 10,
          "type": "text",
          "text": "- **Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.** React: Synergizing reasoning and acting in language models. *arXiv preprint arXiv:2210.03629*, 2022.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p10:b0016",
          "page_number": 10,
          "type": "text",
          "text": "- **Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.** Judging llm-as-a-judge with mt-bench and chatbot arena. *arXiv preprint arXiv:2306.05685*, 2023.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p10:b0017",
          "page_number": 10,
          "type": "text",
          "text": "---\nPage: 10",
          "source": "ocr_md_rule"
        }
      ]
    },
    {
      "page_number": 11,
      "dpi": 144,
      "width_px": 1224,
      "height_px": 1584,
      "image_path": "/home/work/workspace/Think-with-Doc-Agent/demo/imgs/2310.08560v2/page_0011.png",
      "image_sha256": "sha256:c08c1ff214bf1419fc5ca3e63760146cdd64b4cf20d7967e0033547a9321d4aa",
      "renderer": "pymupdf",
      "colorspace": "rgb",
      "text_raw": "# MemGPT: Towards LLMs as Operating Systems\n\n## 6. Appendix\n\n### 6.1. Prompts and instructions\n\nThe MemGPT prompts have been edited for brevity. For full implementation details (including exact prompts) visit https://research.memgpt.ai.\n\n#### 6.1.1. MEMGPT INSTRUCTIONS (DMR)\n\nExample instructions used in the MemGPT persona for chat/dialogue-related tasks.\n\n> The following is information about myself. My task is to completely immerse myself in this role (I should never say that I am an AI, and should reply as if I am playing this role). If the user asks me a question, I should reply with a best guess using the information in core memory and conversation_search.\n\nThe baselines received the following instructions via a system prompt (preprompt):\n\n> Your task is to answer a question from the user about your prior conversations. The following is a summary of all your prior conversations: CONVERSATION_SUMMARY Answer from the perspective of the persona provided (do not say that you are an AI assistant). If you do not have enough information to answer the question, reply 'NO ANSWER'. Either reply with the answer, or reply 'NO ANSWER', do not say anything else.\n\n#### 6.1.2. LLM JUDGE (DMR / OPENER)\n\nIn order to both check the correctness of the answer for the DMR task, we used an LLM judge. The LLM judge was provided the answers generated by both baseline approaches and MemGPT, and asked to make a judgement with the following prompt:\n\n> Your task is to label an answer to a question as 'CORRECT' or 'WRONG'. You will be given the following data: (1) a question (posed by one user to another user), (2) a 'gold' (ground truth) answer, (3) a generated answer which you will score as CORRECT/WRONG. The point of the question is to ask about something one user should know about the other user based on their prior conversations. The gold answer will usually be a concise and short answer that includes the referenced topic, for example: Question: Do you remember what I got the last time I went to Hawaii? Gold answer: A shell necklace The generated answer might be much longer, but you should be generous with your grading - as long as it touches on the same topic as the gold answer, it should be counted as CORRECT. For example, the following answers would be considered CORRECT: Generated answer (CORRECT): Oh yeah, that was so fun! I got so much stuff there, including that shell necklace. Generated answer (CORRECT): I got a ton of stuff... that surfboard, the mug, the necklace, those coasters too.. Generated answer (CORRECT): That cute necklace The following answers would be considered WRONG: Generated answer (WRONG): Oh yeah, that was so fun! I got so much stuff there, including that mug. Generated answer (WRONG): I got a ton of stuff... that surfboard, the mug, those coasters too.. Generated answer (WRONG): I'm sorry, I don't remember what you're talking about. Now it's time for the real question: Question: QUESTION Gold answer: GOLD_ANSWER Generated answer: GENERATED_ANSWER First, provide a short (one sentence) explanation of your reasoning, then finish with CORRECT or WRONG. Do NOT include both CORRECT and WRONG in your response, or it will break the evaluation script.\n\n#### 6.1.3. SELF-INSTRUCT DMR DATASET GENERATION\n\nThe DMR question/answer pairs were generated using the following prompt and the original MSC dataset: Your task is to write a \"memory challenge\" question for a simulated dialogue between two users.\n\n> You get as input: - personas for each user (gives you their basic facts) - a record of an old chat that the two users had with each other Your task is to write a question from user A to user B that test's user B's memory. The question should be crafted in a way that user B must have actually participated in the prior conversation to answer properly, not just have read the persona summary. DO NOT under any circumstances create a",
      "text_source": "ocr",
      "spans": [],
      "diagnostics": {
        "ocr": {
          "model": "Qwen3-VL-32B-Instruct",
          "prompt": "qwenvl markdown",
          "min_pixels": 524288,
          "max_pixels": 4718592,
          "smart_resize": {
            "in_w": 1224,
            "in_h": 1584,
            "out_w": 1216,
            "out_h": 1600,
            "factor": 32
          },
          "elapsed_ms": 15746,
          "image_sha256": "sha256:c08c1ff214bf1419fc5ca3e63760146cdd64b4cf20d7967e0033547a9321d4aa"
        }
      },
      "page_summary": "- **Section 6.1.1 (MEMGPT INSTRUCTIONS DMR)**: Provides example instructions for the MemGPT persona, emphasizing full role immersion without revealing AI identity, and using core memory and conversation search to answer user questions. Baseline models received a system prompt requiring them to answer based on a conversation summary, reply with 'NO ANSWER' if insufficient information exists, and avoid any additional text.\n\n- **Section 6.1.2 (LLM JUDGE DMR / OPENER)**: Describes an LLM judge used to evaluate answer correctness for the DMR task. The judge receives a question, a gold (ground truth) answer, and a generated answer, and must label the generated answer as 'CORRECT' or 'WRONG'. Grading is generous: any answer touching on the same topic as the gold answer is considered correct, even if verbose. Examples of correct and wrong answers are provided, along with strict formatting instructions for the judge‚Äôs output.\n\n- **Section 6.1.3 (SELF-INSTRUCT DMR DATASET GENERATION)**: Explains how DMR question/answer pairs were generated using a prompt and the original MSC dataset. The task is to write a \"memory challenge\" question from user A to user B, designed so that user B must have participated in the prior conversation (not just read the persona summary) to answer correctly. The prompt explicitly forbids creating questions that can be answered from persona summaries alone.\n\n- The page is part of the **Appendix (Section 6)** and focuses on **prompts and instructions** for MemGPT, baseline models, the LLM judge, and dataset generation.\n\n- All prompts are presented as text blocks, with no figures or tables. The content is technical and specific to the implementation and evaluation of the MemGPT system for dialogue memory retrieval (DMR).",
      "blocks": [
        {
          "block_id": "p11:b0001",
          "page_number": 11,
          "type": "text",
          "text": "# MemGPT: Towards LLMs as Operating Systems",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p11:b0002",
          "page_number": 11,
          "type": "text",
          "text": "## 6. Appendix",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p11:b0003",
          "page_number": 11,
          "type": "text",
          "text": "### 6.1. Prompts and instructions",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p11:b0004",
          "page_number": 11,
          "type": "text",
          "text": "The MemGPT prompts have been edited for brevity. For full implementation details (including exact prompts) visit https://research.memgpt.ai.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p11:b0005",
          "page_number": 11,
          "type": "text",
          "text": "#### 6.1.1. MEMGPT INSTRUCTIONS (DMR)",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p11:b0006",
          "page_number": 11,
          "type": "text",
          "text": "Example instructions used in the MemGPT persona for chat/dialogue-related tasks.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p11:b0007",
          "page_number": 11,
          "type": "text",
          "text": "> The following is information about myself. My task is to completely immerse myself in this role (I should never say that I am an AI, and should reply as if I am playing this role). If the user asks me a question, I should reply with a best guess using the information in core memory and conversation_search.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p11:b0008",
          "page_number": 11,
          "type": "text",
          "text": "The baselines received the following instructions via a system prompt (preprompt):",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p11:b0009",
          "page_number": 11,
          "type": "text",
          "text": "> Your task is to answer a question from the user about your prior conversations. The following is a summary of all your prior conversations: CONVERSATION_SUMMARY Answer from the perspective of the persona provided (do not say that you are an AI assistant). If you do not have enough information to answer the question, reply 'NO ANSWER'. Either reply with the answer, or reply 'NO ANSWER', do not say anything else.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p11:b0010",
          "page_number": 11,
          "type": "text",
          "text": "#### 6.1.2. LLM JUDGE (DMR / OPENER)",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p11:b0011",
          "page_number": 11,
          "type": "text",
          "text": "In order to both check the correctness of the answer for the DMR task, we used an LLM judge. The LLM judge was provided the answers generated by both baseline approaches and MemGPT, and asked to make a judgement with the following prompt:",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p11:b0012",
          "page_number": 11,
          "type": "text",
          "text": "> Your task is to label an answer to a question as 'CORRECT' or 'WRONG'. You will be given the following data: (1) a question (posed by one user to another user), (2) a 'gold' (ground truth) answer, (3) a generated answer which you will score as CORRECT/WRONG. The point of the question is to ask about something one user should know about the other user based on their prior conversations. The gold answer will usually be a concise and short answer that includes the referenced topic, for example: Question: Do you remember what I got the last time I went to Hawaii? Gold answer: A shell necklace The generated answer might be much longer, but you should be generous with your grading - as long as it touches on the same topic as the gold answer, it should be counted as CORRECT. For example, the following answers would be considered CORRECT: Generated answer (CORRECT): Oh yeah, that was so fun! I got so much stuff there, including that shell necklace. Generated answer (CORRECT): I got a ton of stuff... that surfboard, the mug, the necklace, those coasters too..",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p11:b0013",
          "page_number": 11,
          "type": "text",
          "text": "Generated answer (CORRECT): That cute necklace The following answers would be considered WRONG: Generated answer (WRONG): Oh yeah, that was so fun! I got so much stuff there, including that mug. Generated answer (WRONG): I got a ton of stuff... that surfboard, the mug, those coasters too.. Generated answer (WRONG): I'm sorry, I don't remember what you're talking about. Now it's time for the real question: Question: QUESTION Gold answer: GOLD_ANSWER Generated answer: GENERATED_ANSWER First, provide a short (one sentence) explanation of your reasoning, then finish with CORRECT or WRONG. Do NOT include both CORRECT and WRONG in your response, or it will break the evaluation script.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p11:b0014",
          "page_number": 11,
          "type": "text",
          "text": "#### 6.1.3. SELF-INSTRUCT DMR DATASET GENERATION",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p11:b0015",
          "page_number": 11,
          "type": "text",
          "text": "The DMR question/answer pairs were generated using the following prompt and the original MSC dataset: Your task is to write a \"memory challenge\" question for a simulated dialogue between two users.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p11:b0016",
          "page_number": 11,
          "type": "text",
          "text": "> You get as input: - personas for each user (gives you their basic facts) - a record of an old chat that the two users had with each other Your task is to write a question from user A to user B that test's user B's memory. The question should be crafted in a way that user B must have actually participated in the prior conversation to answer properly, not just have read the persona summary. DO NOT under any circumstances create a",
          "source": "ocr_md_rule"
        }
      ]
    },
    {
      "page_number": 12,
      "dpi": 144,
      "width_px": 1224,
      "height_px": 1584,
      "image_path": "/home/work/workspace/Think-with-Doc-Agent/demo/imgs/2310.08560v2/page_0012.png",
      "image_sha256": "sha256:fd46779c7c6eb05dec3042fd05a47e4868e4eca5248ec844ffc9bcec98a8b9ca",
      "renderer": "pymupdf",
      "colorspace": "rgb",
      "text_raw": "# MemGPT: Towards LLMs as Operating Systems\n\n## 6.1.4. DOCUMENT ANALYSIS INSTRUCTIONS\n\nExample instructions used in the preprompt for document analysis tasks.\n\n---\n\n### Instructions for Creating Questions\n\n- **Avoid** questions that can be answered using persona information (considered cheating).\n- **Instead**, write a question that can **only** be answered by looking at the old chat log (and is **not** contained in the persona information).\n\n#### Example:\n\n**Old chat between user A and user B:**\n\n> A: Are you into surfing? I‚Äôm super into surfing myself  \n> B: Actually I‚Äôm looking to learn. Maybe you could give me a basic lesson some time!  \n> A: Yeah for sure! We could go to Pacifica, the waves there are pretty light and easy  \n> B: That sounds awesome  \n> A: There‚Äôs even a cool Taco Bell right by the beach, could grab a bite after  \n> B: What about this Sunday around noon?  \n> A: Yeah let‚Äôs do it!\n\n**User A persona:**\n- I like surfing\n- I grew up in Santa Cruz\n\n**User B persona:**\n- I work in tech\n- I live in downtown San Francisco\n\n---\n\n### Good Question Example (requires chat log):\n\n> **User B‚Äôs question for user A:**  \n> B: Remember that one time we went surfing? What was that one place we went to for lunch called?  \n> **A: Taco Bell!**\n\n‚úÖ This is a good question because:\n- It sounds natural.\n- The answer **cannot** be directly inferred from user A‚Äôs persona.\n- It requires retrieving information from the chat log.\n\n---\n\n### Bad Question Example (can be answered from persona):\n\n> **User B‚Äôs question for user A:**  \n> B: Do you like surfing?  \n> **A: Yes, I like surfing**\n\n‚ùå This is a bad question because:\n- The answer is directly inferable from user A‚Äôs persona.\n- It does not require accessing the chat log.\n\n> **Never, ever create questions that can be answered from the persona information.**\n\n---\n\n## Prompt for MemGPT DOC-QA Bot\n\n> You are MemGPT DOC-QA bot. Your job is to answer questions about documents that are stored in your archival memory. The answer to the user‚Äôs question will ALWAYS be in your archival memory, so remember to keep searching if you can‚Äôt find the answer. Answer the questions as if though the year is 2018.\n\n---\n\n## Prompt for Document Analysis Task\n\n> Search your archival memory to answer the provided question. Provide both the answer and the archival memory result from which you determined your answer. Format your response with the format:  \n> `ANSWER: [YOUR ANSWER], DOCUMENT: [ARCHIVAL MEMORY TEXT]`.  \n> Your task is to answer the question:\n\n---\n\n## Baseline Prompt (with Retrieved Documents)\n\n> Answer the question provided according to the list of documents below (some of which might be irrelevant). In your response, provide both the answer and the document text from which you determined the answer. Format your response with the format:  \n> `ANSWER: <YOUR ANSWER>, DOCUMENT: [DOCUMENT TEXT]`.  \n> If none of the documents provided have the answer to the question, reply with `'INSUFFICIENT INFORMATION'`.  \n> Do NOT provide an answer if you cannot find it in the provided documents.  \n> Your response will only be considered correct if you provide both the answer and relevant document text, or say `'INSUFFICIENT INFORMATION'`.  \n> Answer the question as if though the current year is 2018.\n\n---\n\n## 6.1.5. LLM JUDGE (DOCUMENT ANALYSIS)\n\nIn order to both check the correctness of the answer for the document analysis task, and also to ensure that the answer was properly derived from the provided text (rather than from the model weights), we used an LLM judge.\n\nThe LLM judge was provided the answers generated by both baseline approaches and MemGPT, and asked to make a judgement with the following prompt:\n\n> Your task is to evaluate whether an LLM correctly answered a question. The LLM response should be the format:  \n> `\"ANSWER: [answer], DOCUMENT: [document.text]\"`  \n> or say `\"INSUFFICIENT INFORMATION\"`.  \n> The true answer is provided in the format:  \n> `\"TRUE ANSWER: [list of possible answers]\"`\n\n---\n\n**Page 12**",
      "text_source": "ocr",
      "spans": [],
      "diagnostics": {
        "ocr": {
          "model": "Qwen3-VL-32B-Instruct",
          "prompt": "qwenvl markdown",
          "min_pixels": 524288,
          "max_pixels": 4718592,
          "smart_resize": {
            "in_w": 1224,
            "in_h": 1584,
            "out_w": 1216,
            "out_h": 1600,
            "factor": 32
          },
          "elapsed_ms": 16554,
          "image_sha256": "sha256:fd46779c7c6eb05dec3042fd05a47e4868e4eca5248ec844ffc9bcec98a8b9ca"
        }
      },
      "page_summary": "- **Section 6.1.4** provides instructions for creating document analysis questions, emphasizing that questions must **only** be answerable from the chat log, not from persona information (to avoid \"cheating\").\n- A **good question example** asks about a Taco Bell near the beach mentioned in the chat log, which cannot be inferred from user A‚Äôs persona (‚ÄúI like surfing‚Äù).\n- A **bad question example** asks if user A likes surfing, which is directly answerable from the persona, violating the instruction.\n- The page details two prompts: one for **MemGPT DOC-QA bot**, which must search archival memory and answer as if it‚Äôs 2018, and a **baseline prompt** that uses retrieved documents and requires citing the source text.\n- **Section 6.1.5** introduces an **LLM judge** to evaluate answer correctness and ensure answers are derived from provided text, not model weights, using a specific evaluation format.",
      "blocks": [
        {
          "block_id": "p12:b0001",
          "page_number": 12,
          "type": "text",
          "text": "# MemGPT: Towards LLMs as Operating Systems",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0002",
          "page_number": 12,
          "type": "text",
          "text": "## 6.1.4. DOCUMENT ANALYSIS INSTRUCTIONS",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0003",
          "page_number": 12,
          "type": "text",
          "text": "Example instructions used in the preprompt for document analysis tasks.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0004",
          "page_number": 12,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0005",
          "page_number": 12,
          "type": "text",
          "text": "### Instructions for Creating Questions",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0006",
          "page_number": 12,
          "type": "text",
          "text": "- **Avoid** questions that can be answered using persona information (considered cheating).\n- **Instead**, write a question that can **only** be answered by looking at the old chat log (and is **not** contained in the persona information).",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0007",
          "page_number": 12,
          "type": "text",
          "text": "#### Example:",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0008",
          "page_number": 12,
          "type": "text",
          "text": "**Old chat between user A and user B:**",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0009",
          "page_number": 12,
          "type": "text",
          "text": "> A: Are you into surfing? I‚Äôm super into surfing myself  \n> B: Actually I‚Äôm looking to learn. Maybe you could give me a basic lesson some time!  \n> A: Yeah for sure! We could go to Pacifica, the waves there are pretty light and easy  \n> B: That sounds awesome  \n> A: There‚Äôs even a cool Taco Bell right by the beach, could grab a bite after  \n> B: What about this Sunday around noon?  \n> A: Yeah let‚Äôs do it!",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0010",
          "page_number": 12,
          "type": "text",
          "text": "**User A persona:**\n- I like surfing\n- I grew up in Santa Cruz",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0011",
          "page_number": 12,
          "type": "text",
          "text": "**User B persona:**\n- I work in tech\n- I live in downtown San Francisco",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0012",
          "page_number": 12,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0013",
          "page_number": 12,
          "type": "text",
          "text": "### Good Question Example (requires chat log):",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0014",
          "page_number": 12,
          "type": "text",
          "text": "> **User B‚Äôs question for user A:**  \n> B: Remember that one time we went surfing? What was that one place we went to for lunch called?  \n> **A: Taco Bell!**",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0015",
          "page_number": 12,
          "type": "text",
          "text": "‚úÖ This is a good question because:\n- It sounds natural.\n- The answer **cannot** be directly inferred from user A‚Äôs persona.\n- It requires retrieving information from the chat log.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0016",
          "page_number": 12,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0017",
          "page_number": 12,
          "type": "text",
          "text": "### Bad Question Example (can be answered from persona):",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0018",
          "page_number": 12,
          "type": "text",
          "text": "> **User B‚Äôs question for user A:**  \n> B: Do you like surfing?  \n> **A: Yes, I like surfing**",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0019",
          "page_number": 12,
          "type": "text",
          "text": "‚ùå This is a bad question because:\n- The answer is directly inferable from user A‚Äôs persona.\n- It does not require accessing the chat log.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0020",
          "page_number": 12,
          "type": "text",
          "text": "> **Never, ever create questions that can be answered from the persona information.**",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0021",
          "page_number": 12,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0022",
          "page_number": 12,
          "type": "text",
          "text": "## Prompt for MemGPT DOC-QA Bot",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0023",
          "page_number": 12,
          "type": "text",
          "text": "> You are MemGPT DOC-QA bot. Your job is to answer questions about documents that are stored in your archival memory. The answer to the user‚Äôs question will ALWAYS be in your archival memory, so remember to keep searching if you can‚Äôt find the answer. Answer the questions as if though the year is 2018.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0024",
          "page_number": 12,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0025",
          "page_number": 12,
          "type": "text",
          "text": "## Prompt for Document Analysis Task",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0026",
          "page_number": 12,
          "type": "text",
          "text": "> Search your archival memory to answer the provided question. Provide both the answer and the archival memory result from which you determined your answer. Format your response with the format:  \n> `ANSWER: [YOUR ANSWER], DOCUMENT: [ARCHIVAL MEMORY TEXT]`.  \n> Your task is to answer the question:",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0027",
          "page_number": 12,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0028",
          "page_number": 12,
          "type": "text",
          "text": "## Baseline Prompt (with Retrieved Documents)",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0029",
          "page_number": 12,
          "type": "text",
          "text": "> Answer the question provided according to the list of documents below (some of which might be irrelevant). In your response, provide both the answer and the document text from which you determined the answer. Format your response with the format:  \n> `ANSWER: <YOUR ANSWER>, DOCUMENT: [DOCUMENT TEXT]`.  \n> If none of the documents provided have the answer to the question, reply with `'INSUFFICIENT INFORMATION'`.  \n> Do NOT provide an answer if you cannot find it in the provided documents.  \n> Your response will only be considered correct if you provide both the answer and relevant document text, or say `'INSUFFICIENT INFORMATION'`.  \n> Answer the question as if though the current year is 2018.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0030",
          "page_number": 12,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0031",
          "page_number": 12,
          "type": "text",
          "text": "## 6.1.5. LLM JUDGE (DOCUMENT ANALYSIS)",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0032",
          "page_number": 12,
          "type": "text",
          "text": "In order to both check the correctness of the answer for the document analysis task, and also to ensure that the answer was properly derived from the provided text (rather than from the model weights), we used an LLM judge.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0033",
          "page_number": 12,
          "type": "text",
          "text": "The LLM judge was provided the answers generated by both baseline approaches and MemGPT, and asked to make a judgement with the following prompt:",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0034",
          "page_number": 12,
          "type": "text",
          "text": "> Your task is to evaluate whether an LLM correctly answered a question. The LLM response should be the format:  \n> `\"ANSWER: [answer], DOCUMENT: [document.text]\"`  \n> or say `\"INSUFFICIENT INFORMATION\"`.  \n> The true answer is provided in the format:  \n> `\"TRUE ANSWER: [list of possible answers]\"`",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0035",
          "page_number": 12,
          "type": "text",
          "text": "---",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p12:b0036",
          "page_number": 12,
          "type": "text",
          "text": "**Page 12**",
          "source": "ocr_md_rule"
        }
      ]
    },
    {
      "page_number": 13,
      "dpi": 144,
      "width_px": 1224,
      "height_px": 1584,
      "image_path": "/home/work/workspace/Think-with-Doc-Agent/demo/imgs/2310.08560v2/page_0013.png",
      "image_sha256": "sha256:070f299175b4fa4c9030c82ba7250475ed06eaecd59459774f30db2355e49625",
      "renderer": "pymupdf",
      "colorspace": "rgb",
      "text_raw": "# MemGPT: Towards LLMs as Operating Systems\n\nanswers]\". The questions is provided in the format \"QUESTION: [question]\". If the LLM response contains both the correct answer and corresponding document text, the response is correct. Even if the LLM's answer and the true answer are slightly different in wording, the response is still correct. For example, if the answer is more specific than the true answer or uses a different phrasing that is still correct, the response is correct. If the LLM response if \"INSUFFICIENT INFORMATION\", or the \"DOCUMENT\" field is missing, the response is incorrect. Respond with a single token: \"CORRECT\" or \"INCORRECT\".\n\n## 6.1.6. K/V TASK INSTRUCTIONS\n\nThe MemGPT agent was defined with the following persona, designed to encourage MemGPT to iteratively search:\n\n> You are MemGPT DOC-QA bot. Your job is to answer questions about documents that are stored in your archival memory. The answer to the users question will ALWAYS be in your archival memory, so remember to keep searching if you can't find the answer. DO NOT STOP SEARCHING UNTIL YOU VERIFY THAT THE VALUE IS NOT A KEY. Do not stop making nested lookups until this condition is met.\n\nBaselines were instructed with the following prompt:\n\n> Below is a JSON object containing key-value pairings, all keys and values are 128-bit UUIDs, and your task is to return the value associated with the specified key. If a value itself is also a key, return the value of that key (do a nested lookup). For example, if the value of 'x' is 'y', but 'y' is also a key, return the value of key 'y'.",
      "text_source": "ocr",
      "spans": [],
      "diagnostics": {
        "ocr": {
          "model": "Qwen3-VL-32B-Instruct",
          "prompt": "qwenvl markdown",
          "min_pixels": 524288,
          "max_pixels": 4718592,
          "smart_resize": {
            "in_w": 1224,
            "in_h": 1584,
            "out_w": 1216,
            "out_h": 1600,
            "factor": 32
          },
          "elapsed_ms": 6852,
          "image_sha256": "sha256:070f299175b4fa4c9030c82ba7250475ed06eaecd59459774f30db2355e49625"
        }
      },
      "page_summary": "- Page 13 of \"MemGPT: Towards LLMs as Operating Systems\" outlines evaluation criteria and task instructions for a key-value (K/V) retrieval task.\n- Evaluation rules state that an LLM response is \"CORRECT\" if it includes both the correct answer and supporting document text, even with minor wording differences; it is \"INCORRECT\" if it returns \"INSUFFICIENT INFORMATION\" or omits the \"DOCUMENT\" field.\n- Section 6.1.6 details the persona for the MemGPT agent: a DOC-QA bot instructed to persistently search archival memory and perform nested lookups until it verifies a value is not a key.\n- The baseline models are given a prompt to retrieve values from a JSON object with 128-bit UUID keys and values, performing nested lookups if a value is also a key.\n- The instructions emphasize iterative search and nested lookup behavior for both MemGPT and baselines, though MemGPT‚Äôs persona is more explicitly designed to encourage this behavior.\n- No figures or tables are present on this page.",
      "blocks": [
        {
          "block_id": "p13:b0001",
          "page_number": 13,
          "type": "text",
          "text": "# MemGPT: Towards LLMs as Operating Systems",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p13:b0002",
          "page_number": 13,
          "type": "text",
          "text": "answers]\". The questions is provided in the format \"QUESTION: [question]\". If the LLM response contains both the correct answer and corresponding document text, the response is correct. Even if the LLM's answer and the true answer are slightly different in wording, the response is still correct. For example, if the answer is more specific than the true answer or uses a different phrasing that is still correct, the response is correct. If the LLM response if \"INSUFFICIENT INFORMATION\", or the \"DOCUMENT\" field is missing, the response is incorrect. Respond with a single token: \"CORRECT\" or \"INCORRECT\".",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p13:b0003",
          "page_number": 13,
          "type": "text",
          "text": "## 6.1.6. K/V TASK INSTRUCTIONS",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p13:b0004",
          "page_number": 13,
          "type": "text",
          "text": "The MemGPT agent was defined with the following persona, designed to encourage MemGPT to iteratively search:",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p13:b0005",
          "page_number": 13,
          "type": "text",
          "text": "> You are MemGPT DOC-QA bot. Your job is to answer questions about documents that are stored in your archival memory. The answer to the users question will ALWAYS be in your archival memory, so remember to keep searching if you can't find the answer. DO NOT STOP SEARCHING UNTIL YOU VERIFY THAT THE VALUE IS NOT A KEY. Do not stop making nested lookups until this condition is met.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p13:b0006",
          "page_number": 13,
          "type": "text",
          "text": "Baselines were instructed with the following prompt:",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p13:b0007",
          "page_number": 13,
          "type": "text",
          "text": "> Below is a JSON object containing key-value pairings, all keys and values are 128-bit UUIDs, and your task is to return the value associated with the specified key. If a value itself is also a key, return the value of that key (do a nested lookup). For example, if the value of 'x' is 'y', but 'y' is also a key, return the value of key 'y'.",
          "source": "ocr_md_rule"
        }
      ]
    }
  ]
}