# 3. RAG Embedding

**ç°æœ‰å†…å®¹ç¤ºä¾‹ï¼š**

```
{
  "doc_id": "sha256:9f674bcff69c86f11c813dcfad613d8841f5f8ed17979e3c4df06a91df7762e0",
  "source": {
    "type": "local_file",
    "path": "DocAgent/doc/2310.08560v2.pdf"
  },
  "num_pages": 13,
  "metadata": {
    "doc_name": "2310.08560v2.pdf",
    "parser": {
      "renderer": "pymupdf",
      "dpi": 144
    }
  },
  "pages": [
    {
      "page_number": 2,
      "dpi": 144,
      "width_px": 1224,
      "height_px": 1584,
      "image_path": "/Users/yuwenhan/Library/Mobile Documents/com~apple~CloudDocs/Documents/ğŸŸ/ç§‘ç ”/Learn.agent/DocAgent/imgs/2310.08560v2/page_0002.png",
      "image_sha256": "sha256:81169f03539386c246a8f4f55f173309081b6009814f76236702520797d2503e",
      "renderer": "pymupdf",
      "colorspace": "rgb",
      "text_raw": "# MemGPT: Towards LLMs as Operating Systems\n\n<!-- Image (66, 75, 454, 256) -->\nFigure 1. MemGPT (left) writes data to persistent memory after it receives a system alert about limited context space.\n\n<!-- Image (514, 75, 898, 256) -->\nFigure 2. MemGPT (left) can search out-of-context data to bring relevant information into the current context window.\n\nwith *virtual memory*, which provides an illusion of there being more memory resources than are actually available in physical (i.e., main) memory by the OS paging overflow data to disk and retrieving data (via a page fault) back into memory when accessed by applications. To provide a similar illusion of longer context length (analogous to virtual memory), we allow the LLM to manage what is placed in its own context (analogous to physical memory) via an â€˜LLM OSâ€™, which we call MemGPT. MemGPT enables the LLM to retrieve relevant historical data missing from what is placed in-context, and also evict less relevant data from context and into external storage systems. Figure 3 illustrates the components of MemGPT.\n\nThe combined use of a memory-hierarchy, OS functions and event-based control flow allow MemGPT to handle unbounded context using LLMs that have finite context windows. To demonstrate the utility of our new OS-inspired LLM system, we evaluate MemGPT on two domains where the performance of existing LLMs is severely limited by finite context: document analysis, where the length of standard text files can quickly exceed the input capacity of modern LLMs, and conversational agents, where LLMs bound by limited conversation windows lack context awareness, persona consistency, and long-term memory during extended conversations. In both settings, MemGPT is able to overcome the limitations of finite context to outperform existing LLM-based approaches.\n\n## 2. MemGPT (MemoryGPT)\n\nMemGPTâ€™s OS-inspired multi-level memory architecture delineates between two primary memory types: **main context** (analogous to main memory/physical memory/RAM) and **external context** (analogous to disk memory/disk storage). Main context consists of the LLM *prompt tokens*â€”anything in main context is considered *in-context* and can be accessed by the LLM processor during inference. External context refers to any information that is held outside of the LLMs fixed context window. This *out-of-context* data must always be explicitly moved into main context in order for it to be passed to the LLM processor during inference. MemGPT provides function calls that the LLM processor to manage its own memory without any user intervention.\n\n### 2.1. Main context (prompt tokens)\n\nThe prompt tokens in MemGPT are split into three contiguous sections: the **system instructions**, **working context**, and **FIFO Queue**. The system instructions are read-only (static) and contain information on the MemGPT control flow, the intended usage of the different memory levels, and instructions on how to use the MemGPT functions (e.g. how to retrieve out-of-context data). Working context is a fixed-size read/write block of unstructured text, writeable only via MemGPT function calls. In conversational settings, working context is intended to be used to store key facts, preferences, and other important information about the user and the persona the agent is adopting, allowing the agent to converse fluently with the user. The FIFO queue stores a rolling history of messages, including messages between the agent and user, as well as system messages (e.g. memory warnings) and function call inputs and outputs. The first index in the FIFO queue stores a system message containing a recursive summary of messages that have been evicted from the queue.\n\n### 2.2. Queue Manager\n\nThe queue manager manages messages in *recall storage* and the **FIFO queue**. When a new message is received by the system, the queue manager appends the incoming messages to the FIFO queue, concatenates the prompt tokens and triggers the LLM inference to generate LLM output (the completion tokens). The queue manager writes both the incoming message and the generated LLM output to recall storage (the MemGPT message database). When messages in recall storage are retrieved via a MemGPT function call, the queue manager appends them to the back of",
      "text_source": "ocr",
      "spans": [
        {
          "span_id": "2:ocr:region:0001",
          "type": "image",
          "bbox_rel": [
            0.05392156862745098,
            0.04734848484848485,
            0.3709150326797386,
            0.16161616161616163
          ],
          "bbox_px": [
            66.0,
            75.0,
            454.0,
            256.0
          ],
          "source": "ocr"
        },
        {
          "span_id": "2:ocr:region:0002",
          "type": "image",
          "bbox_rel": [
            0.4199346405228758,
            0.04734848484848485,
            0.7336601307189542,
            0.16161616161616163
          ],
          "bbox_px": [
            514.0,
            75.0,
            898.0,
            256.0
          ],
          "source": "ocr"
        }
      ],
      "diagnostics": {
        "ocr": {
          "model": "qwen3-vl-235b-a22b-instruct",
          "prompt": "qwenvl markdown",
          "min_pixels": 524288,
          "max_pixels": 4718592,
          "smart_resize": {
            "in_w": 1224,
            "in_h": 1584,
            "out_w": 1216,
            "out_h": 1600,
            "factor": 32
          },
          "elapsed_ms": 28275,
          "image_sha256": "sha256:81169f03539386c246a8f4f55f173309081b6009814f76236702520797d2503e"
        }
      },
      "page_summary": "- MemGPT is an OS-inspired system that lets LLMs manage memory like an operating system, using â€œmain contextâ€ (in-context prompt tokens) and â€œexternal contextâ€ (out-of-context data stored externally) to simulate virtual memory and overcome finite context limits.\n- Figures 1 and 2 illustrate MemGPTâ€™s core functions: writing data to persistent memory upon system alerts (Fig. 1) and retrieving relevant out-of-context data via search (Fig. 2) to maintain context during long conversations.\n- The main context is divided into three parts: read-only system instructions, a writable working context for key facts/preferences, and a FIFO queue that stores message history and system events, including recursive summaries of evicted messages.\n- The Queue Manager handles message flow by appending new messages to the FIFO queue, triggering LLM inference, and writing both input and output to recall storage (MemGPTâ€™s message database).\n- MemGPT enables unbounded context handling for LLMs, improving performance in document analysis and conversational agents by dynamically managing memory without user intervention.\n- Section 2.2 begins describing the Queue Managerâ€™s role but is cut off mid-sentence on this page.",
      "blocks": [
        {
          "block_id": "p2:b0001",
          "page_number": 2,
          "type": "figure",
          "span_id": "2:ocr:region:0001",
          "bbox_px": [
            80,
            120,
            552,
            410
          ],
          "asset_path": "../chunks/2310.08560v2/page_0002/p0002_2-ocr-region-0001.png",
          "crop_work_size": [
            1216,
            1600
          ],
          "text": "Figure 1. MemGPT (left) writes data to persistent memory after it receives a system alert about limited context space.",
          "source": "ocr_span"
        },
        {
          "block_id": "p2:b0002",
          "page_number": 2,
          "type": "figure",
          "span_id": "2:ocr:region:0002",
          "bbox_px": [
            625,
            120,
            1092,
            410
          ],
          "asset_path": "../chunks/2310.08560v2/page_0002/p0002_2-ocr-region-0002.png",
          "crop_work_size": [
            1216,
            1600
          ],
          "text": "Figure 2. MemGPT (left) can search out-of-context data to bring relevant information into the current context window.",
          "source": "ocr_span"
        },
        {
          "block_id": "p2:b0003",
          "page_number": 2,
          "type": "text",
          "text": "# MemGPT: Towards LLMs as Operating Systems",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0004",
          "page_number": 2,
          "type": "text",
          "text": "Figure 1. MemGPT (left) writes data to persistent memory after it receives a system alert about limited context space.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0005",
          "page_number": 2,
          "type": "text",
          "text": "Figure 2. MemGPT (left) can search out-of-context data to bring relevant information into the current context window.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0006",
          "page_number": 2,
          "type": "text",
          "text": "with *virtual memory*, which provides an illusion of there being more memory resources than are actually available in physical (i.e., main) memory by the OS paging overflow data to disk and retrieving data (via a page fault) back into memory when accessed by applications. To provide a similar illusion of longer context length (analogous to virtual memory), we allow the LLM to manage what is placed in its own context (analogous to physical memory) via an â€˜LLM OSâ€™, which we call MemGPT. MemGPT enables the LLM to retrieve relevant historical data missing from what is placed in-context, and also evict less relevant data from context and into external storage systems. Figure 3 illustrates the components of MemGPT.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0007",
          "page_number": 2,
          "type": "text",
          "text": "The combined use of a memory-hierarchy, OS functions and event-based control flow allow MemGPT to handle unbounded context using LLMs that have finite context windows. To demonstrate the utility of our new OS-inspired LLM system, we evaluate MemGPT on two domains where the performance of existing LLMs is severely limited by finite context: document analysis, where the length of standard text files can quickly exceed the input capacity of modern LLMs, and conversational agents, where LLMs bound by limited conversation windows lack context awareness, persona consistency, and long-term memory during extended conversations. In both settings, MemGPT is able to overcome the limitations of finite context to outperform existing LLM-based approaches.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0008",
          "page_number": 2,
          "type": "text",
          "text": "## 2. MemGPT (MemoryGPT)",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0009",
          "page_number": 2,
          "type": "text",
          "text": "MemGPTâ€™s OS-inspired multi-level memory architecture delineates between two primary memory types: **main context** (analogous to main memory/physical memory/RAM) and **external context** (analogous to disk memory/disk storage). Main context consists of the LLM *prompt tokens*â€”anything in main context is considered *in-context* and can be accessed by the LLM processor during inference. External context refers to any information that is held outside of the LLMs fixed context window. This *out-of-context* data must always be explicitly moved into main context in order for it to be passed to the LLM processor during inference. MemGPT provides function calls that the LLM processor to manage its own memory without any user intervention.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0010",
          "page_number": 2,
          "type": "text",
          "text": "### 2.1. Main context (prompt tokens)",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0011",
          "page_number": 2,
          "type": "text",
          "text": "The prompt tokens in MemGPT are split into three contiguous sections: the **system instructions**, **working context**, and **FIFO Queue**. The system instructions are read-only (static) and contain information on the MemGPT control flow, the intended usage of the different memory levels, and instructions on how to use the MemGPT functions (e.g. how to retrieve out-of-context data). Working context is a fixed-size read/write block of unstructured text, writeable only via MemGPT function calls. In conversational settings, working context is intended to be used to store key facts, preferences, and other important information about the user and the persona the agent is adopting, allowing the agent to converse fluently with the user. The FIFO queue stores a rolling history of messages, including messages between the agent and user, as well as system messages (e.g. memory warnings) and function call inputs and outputs. The first index in the FIFO queue stores a system message containing a recursive summary of messages that have been evicted from the queue.",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0012",
          "page_number": 2,
          "type": "text",
          "text": "### 2.2. Queue Manager",
          "source": "ocr_md_rule"
        },
        {
          "block_id": "p2:b0013",
          "page_number": 2,
          "type": "text",
          "text": "The queue manager manages messages in *recall storage* and the **FIFO queue**. When a new message is received by the system, the queue manager appends the incoming messages to the FIFO queue, concatenates the prompt tokens and triggers the LLM inference to generate LLM output (the completion tokens). The queue manager writes both the incoming message and the generated LLM output to recall storage (the MemGPT message database). When messages in recall storage are retrieved via a MemGPT function call, the queue manager appends them to the back of",
          "source": "ocr_md_rule"
        }
      ]
    }
]
}
```
* è‡³æ­¤ï¼Œpdfåˆæ­¥å¤„ç†å·²ç»å®Œæˆäº†ï¼Œä¸€ä¸ªpdfå·²ç»è¢«æ‹†åˆ†ä¸º  **å›¾åƒã€è¡¨æ ¼ã€æ–‡æœ¬**
* æ¯ä¸€é¡µåŒ…å«ï¼š **summaryã€chunks**

**ä¸‹ä¸€æ­¥â€œ**

åŸºäºå·²ç»åˆ‡åˆ†å¥½çš„chunkså’Œsummaryåšembeddingï¼Œç”¨äºåç»­çš„æ£€ç´¢ã€‚



## è§„åˆ’
### **1) å®šä¹‰â€œç´¢å¼•è¾“å…¥å•å…ƒâ€ä¸ ID è§„èŒƒï¼ˆå…ˆå®šæ¥å£ï¼Œåé¢æ‰ç¨³ï¼‰**
ä½ ç°åœ¨ blocks ç²’åº¦å·²ç»å¤Ÿç”¨ï¼Œå»ºè®®ç›´æ¥æŠŠæ¯ä¸ª block ä½œä¸ºæœ€å°ç´¢å¼•å•å…ƒï¼ˆåç»­å†åš chunk merge ä¹Ÿä¸è¿Ÿï¼‰ï¼Œå¹¶ç»Ÿä¸€ç”Ÿæˆï¼š

* chunk_idï¼šå»ºè®®æ²¿ç”¨ block_idï¼ˆå¦‚ p5:b0012ï¼‰ï¼Œä¿è¯å…¨å±€å”¯ä¸€
* modalityï¼štext | image | table
* payloadï¼š
    * textï¼štext
    * image/tableï¼šasset_pathï¼ˆä»¥åŠå¯é€‰ caption/textï¼Œå¦‚æœä½ æ„¿æ„åšâ€œå›¾åƒæ£€ç´¢ + æ–‡æœ¬è¿‡æ»¤â€ï¼‰

* page_numberã€bbox_pxã€crop_work_sizeï¼šç”¨äºå›é“¾ä¸å¯è§†åŒ–å®šä½
* source_doc_idï¼šä½ å·²æœ‰ doc_id

è¿™ä¸€æ­¥çš„äº§ç‰©æ˜¯ä¸€ä¸ªæ‰å¹³åŒ–åˆ—è¡¨ï¼šchunks[]ï¼Œè€Œä¸æ˜¯ page åµŒå¥—ç»“æ„ï¼ˆæ£€ç´¢ä¸å‘é‡åº“æ›´å‹å¥½ï¼‰ã€‚

### **2) è®¡ç®— embeddingï¼ˆæŒ‰ä½ è¯´çš„ï¼šæ–‡æœ¬ä¸å†å¤„ç†ï¼›å›¾/è¡¨ç”¨å›¾åƒ embeddingï¼‰**
å»ºè®®ç­–ç•¥ï¼š

* **Text blocks**ï¼šç›´æ¥å¯¹ block.text åš text embedding
* **Figure/Table blocks**ï¼šè¯»å– asset_path æŒ‡å‘çš„è£å‰ªå›¾ï¼Œåš image embedding
    * è‹¥ä½ æœ€ç»ˆå¸Œæœ›â€œæ–‡æœ¬é—®å›¾/è¡¨â€ï¼Œå¯ä»¥é¢å¤–åšä¸€ä¸ªè½»é‡çš„ caption_embeddingï¼ˆç”¨ä½ å·²æœ‰çš„ block.text æˆ– page_summaryï¼‰ï¼Œä½†è¿™ä¸æ˜¯å¿…é¡»


è½åœ°ç»“æœï¼šä¸ºæ¯ä¸ª chunk_id äº§å‡º

* embedding: float[]
* embedding_modelã€dimã€created_at
* modality

### **3) å»ºç«‹å‘é‡ç´¢å¼•ä¸å­˜å‚¨ï¼ˆå…ˆè·‘é€šï¼Œå†ä¼˜åŒ–ï¼‰**
æœ€ä½æˆæœ¬è·¯çº¿ï¼š

* æœ¬åœ°ï¼šFAISSï¼ˆæˆ– hnswlibï¼‰+ ä¸€ä¸ª chunks_meta.jsonl
* æˆ–ä½ å¦‚æœåå·¥ç¨‹åŒ–ï¼šPostgreSQL + pgvectorï¼ˆå’Œ MemGPT çš„æ€è·¯ä¸€è‡´ï¼‰

è‡³å°‘éœ€è¦ä¸¤ä¸ªç´¢å¼•ï¼š

* text_indexï¼šåªæ”¶ text embeddings
* vision_indexï¼šåªæ”¶ image/table embeddingsï¼ˆè¿™æ ·æŸ¥è¯¢æ—¶ä¸ä¼šæ··æ·†ç©ºé—´ï¼›åç»­å†åšèåˆ re-rankingï¼‰

### **4) åšä¸€ä¸ªæœ€å°å¯ç”¨çš„æ£€ç´¢ APIï¼ˆä½ å¾ˆå¿«å°±èƒ½éªŒæ”¶ï¼‰**
å®ç° 3 ä¸ªæŸ¥è¯¢å‡½æ•°å°±èƒ½ Demoï¼š

1. search_text(query, topk) â†’ è¿”å› text blocks
2. search_image(query_image, topk) â†’ è¿”å› figure/table blocks
3. search_hybrid(query_text, topk_text, topk_img) â†’ åˆå¹¶ä¸¤è·¯ç»“æœï¼ˆç®€å•åŠ æƒæˆ–ä¸²è”ï¼‰

è¿”å›ç»“æœé‡Œå¿…é¡»å¸¦ï¼š

* chunk_id, page_number, bbox_px, asset_path/textè¿™æ ·ä½ å‰ç«¯æˆ– notebook å¯ä»¥ç›´æ¥å®šä½åˆ°å›¾/è¡¨è£å‰ªå›¾ï¼Œå½¢æˆâ€œå¯æº¯æºâ€çš„ doc agent ä½“éªŒã€‚

### **5) åšâ€œå¤šæ¨¡æ€èåˆâ€æœ€å°å®ç°**
ä½ è¯´è¦â€œå¤šæ¨¡æ€ embedding ç»“åˆâ€ï¼Œå»ºè®®å…ˆç”¨ç®€å•å¯é çš„èåˆç­–ç•¥ï¼Œä¸è¦ä¸€å¼€å§‹å°±ä¸Šå¤æ‚æ¨¡å‹ï¼š

* **Late Fusionï¼ˆæ¨èï¼‰**ï¼šä¸¤è·¯æ£€ç´¢å„å– topKï¼Œåšå½’ä¸€åŒ–åˆ†æ•°ååŠ æƒï¼š
    * score = w_text * sim_text + w_img * sim_img

* **Two-stage**ï¼šå…ˆæ–‡æœ¬å¬å›ï¼ˆpage/sectionï¼‰ï¼Œå†åœ¨è¿™äº›é¡µå†…åšå›¾åƒå¬å›ï¼ˆæˆ–åè¿‡æ¥ï¼‰

è¿™ä¸€æ­¥ä¸éœ€è¦æ”¹ä½  Step2 çš„ JSON ç»“æ„ï¼Œåªåœ¨æ£€ç´¢å±‚åšé€»è¾‘å³å¯ã€‚



## åŒç´¢å¼•æ–¹æ¡ˆï¼š**ç›®æ ‡ç´¢å¼•è®¾è®¡**
### **A. Summary Indexï¼ˆç²—å¬å› / è·¯ç”±ï¼‰**
**ç´¢å¼•å•å…ƒï¼špage_summaryï¼ˆæˆ–ä½ åç»­çš„ section_summaryï¼‰**

* doc_id
* page_number
* summary_textï¼ˆä½  JSON é‡Œå·²æœ‰ page_summaryï¼‰
* embeddingï¼ˆç”¨ä½ çš„å¤šæ¨¡æ€ APIï¼Œä½†è¿™é‡Œè¾“å…¥å°±æ˜¯çº¯æ–‡æœ¬ï¼‰

ç”¨é€”ï¼š

* ä½œä¸ºç¬¬ä¸€é˜¶æ®µå¬å›ï¼šå…ˆå®šä½â€œå¯èƒ½ç›¸å…³çš„é¡µ/ç« èŠ‚â€
* é™ä½åç»­ chunk æ£€ç´¢çš„æœç´¢ç©ºé—´ï¼Œæé«˜ç¨³å®šæ€§ä¸é€Ÿåº¦

### **B. Chunk Indexï¼ˆç»†å¬å› / è¯æ®ï¼‰**
**ç´¢å¼•å•å…ƒï¼šchunkï¼ˆè¯­ä¹‰å—ï¼‰ï¼Œchunk é‡ŒåŒ…å«æ–‡å­— + å›¾åƒ**

ä½ ç°åœ¨çš„ JSON blocks å·²ç»æœ‰ text / figure / tableï¼Œå»ºè®®å…ˆåšä¸€ä¸ªâ€œé¡µé¢å†…èšåˆâ€çš„ chunkï¼ˆä¸ç”¨åšå¤æ‚è¯­ä¹‰åˆ†æ®µï¼Œå…ˆè·‘é€šï¼‰ï¼š

**chunk çš„ç»„æˆï¼ˆæ¨è v1ï¼‰ï¼šä»¥ page ä¸ºè¾¹ç•Œèšåˆ**

* chunk_id = f"p{page_number}:c0001"ï¼ˆæ¯é¡µä¸€ä¸ª chunkï¼Œå…ˆæœ€ç®€å•ï¼‰
* text = page çš„æ‰€æœ‰ text blocks æ‹¼æ¥ï¼ˆä¿æŒåŸé¡ºåºï¼‰
* images = è¯¥é¡µæ‰€æœ‰ figure/table çš„ asset_path åˆ—è¡¨
* page_number
* source_blocks = [block_id...]ï¼ˆç”¨äºå›é“¾ï¼‰

**embedding è¾“å…¥ï¼š**

* å¦‚æœä½ çš„å¤šæ¨¡æ€ API æ”¯æŒâ€œtext + imagesâ€è”åˆè¾“å…¥ï¼šç›´æ¥ä¸€æ¬¡è°ƒç”¨å¾—åˆ°ä¸€ä¸ªå‘é‡ã€‚
* å¦‚æœåªæ”¯æŒå•è¾“å…¥ï¼šä¹Ÿå¯ä»¥åšï¼ˆtext_embedding ä¸ image_embeddingï¼‰å†åœ¨ä½ è¿™è¾¹åšèåˆï¼Œä½†ä½ è¯´ API å¯å¤šæ¨¡æ€ä¸€è‡´è¾“å‡ºï¼Œæˆ‘é»˜è®¤å®ƒæ”¯æŒè”åˆè¾“å…¥ã€‚

ç”¨é€”ï¼š

* ç”¨äºç¬¬äºŒé˜¶æ®µç²¾æ£€ç´¢ï¼šåœ¨å€™é€‰é¡µå†…æ‰¾åˆ°æœ€ç›¸å…³ chunk
* è¿”å› chunk åå¯ä»¥å†å±•å¼€åˆ° block çº§ï¼ˆbboxã€asset_pathï¼‰åšå®šä½/é«˜äº®

## **æ£€ç´¢æµç¨‹**
1. ç”¨ Summary Index å¯¹ query åš topKï¼ˆæ¯”å¦‚ 3~5 é¡µï¼‰
2. åœ¨è¿™äº›é¡µå¯¹åº”çš„ chunk èŒƒå›´å†…ï¼Œç”¨ Chunk Index å†åš topKï¼ˆæ¯”å¦‚ 5~10ï¼‰
3. è¾“å‡ºï¼šchunk + å…¶å…³è” blocksï¼ˆå›¾/è¡¨è£å‰ªå›¾è·¯å¾„ã€bboxã€é¡µç ï¼‰

è¿™ä¼šéå¸¸ç¨³ï¼Œä¸”æ˜“äºåç»­æ‰©å±•åˆ°â€œsection summary / section chunksâ€ã€‚
